{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pollyjuice74/Error-Bit-Decoding/blob/main/DDECCT_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_k_fbfIf91j",
        "outputId": "e61eb92c-32f3-4a0c-99b3-c74bcd83cd7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DDECC'...\n",
            "remote: Enumerating objects: 278, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 278 (delta 3), reused 6 (delta 2), pack-reused 268\u001b[K\n",
            "Receiving objects: 100% (278/278), 975.03 MiB | 25.04 MiB/s, done.\n",
            "Resolving deltas: 100% (88/88), done.\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils import data\n",
        "from datetime import datetime\n",
        "import time\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "if not os.path.exists('DDECC'):\n",
        "  !git clone https://github.com/pollyjuice74/DDECC.git\n",
        "os.chdir('DDECC')\n",
        "\n",
        "from Codes import *\n",
        "from DDECC import DDECCT\n",
        "from utils import *\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The modifications are made so that the model trains on BCH codes of length n=63, k=45, where it says:\n",
        "\n",
        "\"### IMPORTANT ###\""
      ],
      "metadata": {
        "id": "pGW0jSfZQ5vg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI-xffVWgJWo",
        "outputId": "87fc8cfb-3199-4db5-a7a2-3ab0aec232e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to model/logs: DDECCT_Results/BCH__Code_n_63_k_45\n",
            "Args: Namespace(code_type='BCH', code_k=45, code_n=63, epochs=2000, workers=4, lr=0.0005, gpus='0', batch_size=128, test_batch_size=2048, seed=42, N_dec=2, d_model=32, h=8, sigma=0.01, code=<__main__.Code object at 0x7be33a2464d0>, N_steps=23, path='DDECCT_Results/BCH__Code_n_63_k_45')\n"
          ]
        }
      ],
      "source": [
        "# Setup the argument parser\n",
        "parser = argparse.ArgumentParser(description='PyTorch DDPM_ECCT')\n",
        "\n",
        "### IMPORTANT ###\n",
        "###############################################################################\n",
        "###############################################################################\n",
        "\n",
        "# select code type, k and n                           ###\n",
        "parser.add_argument('--code_type', type=str, default='BCH', choices=['BCH', 'POLAR', 'LDPC', 'CCSDS', 'MACKAY'])\n",
        "parser.add_argument('--code_k', type=int, default=45) # k\n",
        "parser.add_argument('--code_n', type=int, default=63) # n\n",
        "\n",
        "###############################################################################\n",
        "###############################################################################\n",
        "\n",
        "parser.add_argument('--epochs', type=int, default=2000) ## EPOCHS\n",
        "parser.add_argument('--workers', type=int, default=4)\n",
        "parser.add_argument('--lr', type=float, default=5e-4)\n",
        "parser.add_argument('--gpus', type=str, default='0', help='gpus ids')\n",
        "parser.add_argument('--batch_size', type=int, default=128)\n",
        "parser.add_argument('--test_batch_size', type=int, default=2048)\n",
        "parser.add_argument('--seed', type=int, default=42)\n",
        "parser.add_argument('--N_dec', type=int, default=2)\n",
        "parser.add_argument('--d_model', type=int, default=32)\n",
        "parser.add_argument('--h', type=int, default=8)\n",
        "parser.add_argument('--sigma', type=float, default=0.01)\n",
        "\n",
        "# Function to set seed for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "# Adjust argument parsing for notebook environments\n",
        "if 'ipykernel' in sys.argv[0] or 'colab' in sys.argv[0]:\n",
        "    args = parser.parse_args(args=[])\n",
        "else:\n",
        "    args = parser.parse_args()\n",
        "\n",
        "# Environment settings for CUDA\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpus\n",
        "\n",
        "# Apply the seed\n",
        "set_seed(args.seed)\n",
        "\n",
        "# Code setup\n",
        "class Code():\n",
        "    pass\n",
        "\n",
        "code = Code()\n",
        "code.k = args.code_k\n",
        "code.n = args.code_n\n",
        "code.code_type = args.code_type\n",
        "G, H = Get_Generator_and_Parity(code)\n",
        "code.generator_matrix = torch.from_numpy(G).transpose(0, 1).long()\n",
        "code.pc_matrix = torch.from_numpy(H).long()\n",
        "args.code = code\n",
        "args.N_steps = code.pc_matrix.shape[0] + 5  # Calculate steps\n",
        "\n",
        "# Setup model directory and\n",
        "model_dir = os.path.join('DDECCT_Results', f'{args.code_type}__Code_n_{args.code_n}_k_{args.code_k}') #__{datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")}')\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "args.path = model_dir\n",
        "\n",
        "print(f\"Path to model/logs: {model_dir}\")\n",
        "print(f\"Args: {args}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The modifications made are seen in the **FEC_Dataset** where it says:\n",
        "\n",
        "\"### IMPORTANT ###\""
      ],
      "metadata": {
        "id": "cc9SRvk2QS3a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QXuKlYv-n6Vj"
      },
      "outputs": [],
      "source": [
        "############ ZERO CODEWORD SET TO TRUE ###############\n",
        "class FEC_Dataset(data.Dataset):                 ####\n",
        "    def __init__(self, code, sigma, len, zero_cw=True):\n",
        "        self.code = code\n",
        "        self.sigma = sigma\n",
        "        self.len = len\n",
        "        self.generator_matrix = code.generator_matrix.transpose(0, 1)\n",
        "        self.pc_matrix = code.pc_matrix.transpose(0, 1)\n",
        "\n",
        "        self.zero_word = torch.zeros((self.code.k)).long() if zero_cw else None\n",
        "        self.zero_cw = torch.zeros((self.code.n)).long() if zero_cw else None\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.zero_cw is None:\n",
        "            m = torch.randint(0, 2, (1, self.code.k)).squeeze()\n",
        "            x = torch.matmul(m, self.generator_matrix) % 2\n",
        "        else: # SET TO TRUE\n",
        "            m = self.zero_word\n",
        "            x = self.zero_cw\n",
        "\n",
        "        std_noise = random.choice(self.sigma)\n",
        "        z = torch.randn(self.code.n) * std_noise\n",
        "        #h = torch.from_numpy(np.random.rayleigh(1,self.code.n)).float()\n",
        "        # h=1\n",
        "        # y = h*bin_to_sign(x) + z\n",
        "\n",
        "        ### IMPORTANT ###\n",
        "        #######################################################################\n",
        "        #######################################################################\n",
        "        y = x.clone()\n",
        "\n",
        "        # index to be flipped\n",
        "        ix = torch.tensor(random.sample(range(self.code.n), 3))\n",
        "        y[ix] = 1 - y[ix] # flip bits\n",
        "        y = bin_to_sign(y)\n",
        "\n",
        "        #######################################################################\n",
        "        #######################################################################\n",
        "\n",
        "        magnitude = torch.abs(y)\n",
        "        syndrome = torch.matmul(sign_to_bin(torch.sign(y)).long(),\n",
        "                                self.pc_matrix) % 2\n",
        "        syndrome = bin_to_sign(syndrome)\n",
        "        return m.float(), x.float(), z.float(), y.float(), magnitude.float(), syndrome.float()\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch, LR):\n",
        "    model.train()\n",
        "    cum_loss = cum_samples = 0\n",
        "    t = time.time()\n",
        "    for batch_idx, (m, x, z, y, magnitude, syndrome) in enumerate(\n",
        "            train_loader):\n",
        "        loss = model.loss(bin_to_sign(x))\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        model.ema.update(model)\n",
        "        ###\n",
        "        cum_loss += loss.item() * x.shape[0]\n",
        "        cum_samples += x.shape[0]\n",
        "        if (batch_idx+1) % 500 == 0 or batch_idx == len(train_loader) - 1:\n",
        "            print(\n",
        "                f'Training epoch {epoch}, Batch {batch_idx + 1}/{len(train_loader)}: LR={LR:.2e}, Loss={cum_loss / cum_samples:.5e}')\n",
        "    print(f'Epoch {epoch} Train Time {time.time() - t}s\\n')\n",
        "    return cum_loss / cum_samples\n",
        "\n",
        "##################################################################\n",
        "\n",
        "def test(model, device, test_loader_list, EbNo_range_test, min_FER=100, max_cum_count=1e7, min_cum_count=1e5):\n",
        "    model.eval()\n",
        "    test_loss_ber_list, test_loss_fer_list, cum_samples_all = [], [], []\n",
        "    t = time.time()\n",
        "    with torch.no_grad():\n",
        "        for ii, test_loader in enumerate(test_loader_list):\n",
        "            test_ber = test_fer = cum_count = 0.\n",
        "            _, x_pred_list, _, _ = model.p_sample_loop(next(iter(test_loader))[3])\n",
        "            test_ber_ddpm , test_fer_ddpm = [0]*len(x_pred_list), [0]*len(x_pred_list)\n",
        "            idx_conv_all = []\n",
        "            while True:\n",
        "                (m, x, z, y, magnitude, syndrome) = next(iter(test_loader))\n",
        "                x_pred, x_pred_list, idx_conv,synd_all = model.p_sample_loop(y)\n",
        "                x_pred = sign_to_bin(torch.sign(x_pred))\n",
        "\n",
        "                idx_conv_all.append(idx_conv)\n",
        "                for kk, x_pred_tmp in enumerate(x_pred_list):\n",
        "                    x_pred_tmp = sign_to_bin(torch.sign(x_pred_tmp))\n",
        "\n",
        "                    test_ber_ddpm[kk] += BER(x_pred_tmp, x) * x.shape[0]\n",
        "                    test_fer_ddpm[kk] += FER(x_pred_tmp, x) * x.shape[0]\n",
        "\n",
        "                test_ber += BER(x_pred, x) * x.shape[0]\n",
        "                test_fer += FER(x_pred, x) * x.shape[0]\n",
        "                cum_count += x.shape[0]\n",
        "                if (min_FER > 0 and test_fer > min_FER and cum_count > min_cum_count) or cum_count >= max_cum_count:\n",
        "                    if cum_count >= 1e9:\n",
        "                        print(f'Cum count reached EbN0:{EbNo_range_test[ii]}')\n",
        "                    else:\n",
        "                        print(f'FER count treshold reached EbN0:{EbNo_range_test[ii]}')\n",
        "                    break\n",
        "            idx_conv_all = torch.stack(idx_conv_all).float()\n",
        "            cum_samples_all.append(cum_count)\n",
        "            test_loss_ber_list.append(test_ber / cum_count)\n",
        "            test_loss_fer_list.append(test_fer / cum_count)\n",
        "            for kk in range(len(test_ber_ddpm)):\n",
        "                test_ber_ddpm[kk] /= cum_count\n",
        "                test_fer_ddpm[kk] /= cum_count\n",
        "            print(f'Test EbN0={EbNo_range_test[ii]}, BER={test_loss_ber_list}')\n",
        "            print(f'Test EbN0={EbNo_range_test[ii]}, BER_DDPM={test_ber_ddpm}')\n",
        "            print(f'Test EbN0={EbNo_range_test[ii]}, -ln(BER)_DDPM={[-np.log(elem) for elem in test_ber_ddpm]}')\n",
        "            print(f'Test EbN0={EbNo_range_test[ii]}, FER_DDPM={test_fer_ddpm}')\n",
        "            print(f'#It. to zero syndrome: Mean={idx_conv_all.mean()}, Std={idx_conv_all.std()}, Min={idx_conv_all.min()}, Max={idx_conv_all.max()}')\n",
        "        ###\n",
        "        print('Test FER ' + ' '.join(\n",
        "            ['{}: {:.2e}'.format(ebno, elem) for (elem, ebno)\n",
        "             in\n",
        "             (zip(test_loss_fer_list, EbNo_range_test))]))\n",
        "        print('Test BER ' + ' '.join(\n",
        "            ['{}: {:.2e}'.format(ebno, elem) for (elem, ebno)\n",
        "             in\n",
        "             (zip(test_loss_ber_list, EbNo_range_test))]))\n",
        "        print('Test -ln(BER) ' + ' '.join(\n",
        "            ['{}: {:.2e}'.format(ebno, -np.log(elem)) for (elem, ebno)\n",
        "             in\n",
        "             (zip(test_loss_ber_list, EbNo_range_test))]))\n",
        "    print(f'# of testing samples: {cum_samples_all}\\n Test Time {time.time() - t} s\\n')\n",
        "    return test_loss_ber_list, test_loss_fer_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "f4VhjwNEljG3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b7df3d75-81ed-4d0d-c0e7-b5aec7992618"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Best Model\n",
            "DDECCT_Results/BCH__Code_n_63_k_45\n",
            "/content/DDECC\n",
            "DDECCT(\n",
            "  (decoder): Encoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-1): 2 x EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linears): ModuleList(\n",
            "            (0-3): 4 x Linear(in_features=32, out_features=32, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=32, out_features=128, bias=True)\n",
            "          (w_2): Linear(in_features=128, out_features=32, bias=True)\n",
            "          (dropout): Dropout(p=0, inplace=False)\n",
            "        )\n",
            "        (sublayer): ModuleList(\n",
            "          (0-1): 2 x SublayerConnection(\n",
            "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "    (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (oned_final_embed): Sequential(\n",
            "    (0): Linear(in_features=32, out_features=1, bias=True)\n",
            "  )\n",
            "  (out_fc): Linear(in_features=81, out_features=63, bias=True)\n",
            "  (time_embed): Embedding(23, 32)\n",
            ")\n",
            "# of Parameters: 34063\n",
            "Training model with code type: BCH\n",
            "BCH\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 1, Batch 500/1000: LR=5.00e-04, Loss=1.12574e-03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 1, Batch 1000/1000: LR=5.00e-04, Loss=1.13861e-03\n",
            "Epoch 1 Train Time 36.68302035331726s\n",
            "\n",
            "Model Saved\n",
            "[main 9b31bf2] Add trained model weights\n",
            " 4 files changed, 0 insertions(+), 0 deletions(-)\n",
            "Enumerating objects: 15, done.\n",
            "Counting objects: 100% (15/15), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (7/7), done.\n",
            "Writing objects: 100% (9/9), 27.76 MiB | 8.50 MiB/s, done.\n",
            "Total 9 (delta 2), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
            "remote: \u001b[1;33mwarning\u001b[m: See https://gh.io/lfs for more information.\u001b[K\n",
            "remote: \u001b[1;33mwarning\u001b[m: File DDECCT_Results/BCH__Code_n_63_k_45/best_model is 51.57 MB; this is larger than GitHub's recommended maximum file size of 50.00 MB\u001b[K\n",
            "remote: \u001b[1;33mwarning\u001b[m: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.\u001b[K\n",
            "To https://github.com/pollyjuice74/DDECC.git\n",
            "   dbaa521..9b31bf2  main -> main\n",
            "Training epoch 2, Batch 500/1000: LR=5.00e-04, Loss=1.13928e-03\n",
            "Training epoch 2, Batch 1000/1000: LR=5.00e-04, Loss=1.15471e-03\n",
            "Epoch 2 Train Time 35.602675676345825s\n",
            "\n",
            "Training epoch 3, Batch 500/1000: LR=5.00e-04, Loss=1.14722e-03\n",
            "Training epoch 3, Batch 1000/1000: LR=5.00e-04, Loss=1.14589e-03\n",
            "Epoch 3 Train Time 35.030702352523804s\n",
            "\n",
            "Training epoch 4, Batch 500/1000: LR=5.00e-04, Loss=1.10895e-03\n",
            "Training epoch 4, Batch 1000/1000: LR=5.00e-04, Loss=1.13429e-03\n",
            "Epoch 4 Train Time 35.25141215324402s\n",
            "\n",
            "Model Saved\n",
            "Training epoch 5, Batch 500/1000: LR=5.00e-04, Loss=1.16494e-03\n",
            "Training epoch 5, Batch 1000/1000: LR=5.00e-04, Loss=1.14945e-03\n",
            "Epoch 5 Train Time 34.158162355422974s\n",
            "\n",
            "[main 9f2ac9b] Add trained model weights\n",
            " 1 file changed, 0 insertions(+), 0 deletions(-)\n",
            "Enumerating objects: 9, done.\n",
            "Counting objects: 100% (9/9), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (3/3), done.\n",
            "Writing objects: 100% (5/5), 27.73 MiB | 8.81 MiB/s, done.\n",
            "Total 5 (delta 1), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n",
            "remote: \u001b[1;33mwarning\u001b[m: See https://gh.io/lfs for more information.\u001b[K\n",
            "remote: \u001b[1;33mwarning\u001b[m: File DDECCT_Results/BCH__Code_n_63_k_45/best_model is 51.57 MB; this is larger than GitHub's recommended maximum file size of 50.00 MB\u001b[K\n",
            "remote: \u001b[1;33mwarning\u001b[m: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.\u001b[K\n",
            "To https://github.com/pollyjuice74/DDECC.git\n",
            "   9b31bf2..9f2ac9b  main -> main\n",
            "Training epoch 6, Batch 500/1000: LR=5.00e-04, Loss=1.23923e-03\n",
            "Training epoch 6, Batch 1000/1000: LR=5.00e-04, Loss=1.19035e-03\n",
            "Epoch 6 Train Time 34.556556701660156s\n",
            "\n",
            "Training epoch 7, Batch 500/1000: LR=5.00e-04, Loss=1.20845e-03\n",
            "Training epoch 7, Batch 1000/1000: LR=5.00e-04, Loss=1.21251e-03\n",
            "Epoch 7 Train Time 35.23333382606506s\n",
            "\n",
            "Training epoch 8, Batch 500/1000: LR=5.00e-04, Loss=1.10307e-03\n",
            "Training epoch 8, Batch 1000/1000: LR=5.00e-04, Loss=1.12328e-03\n",
            "Epoch 8 Train Time 34.765352964401245s\n",
            "\n",
            "Model Saved\n",
            "Training epoch 9, Batch 500/1000: LR=5.00e-04, Loss=1.17862e-03\n",
            "Training epoch 9, Batch 1000/1000: LR=5.00e-04, Loss=1.22323e-03\n",
            "Epoch 9 Train Time 33.70787191390991s\n",
            "\n",
            "Training epoch 10, Batch 500/1000: LR=5.00e-04, Loss=1.25584e-03\n",
            "Training epoch 10, Batch 1000/1000: LR=5.00e-04, Loss=1.23052e-03\n",
            "Epoch 10 Train Time 34.91365194320679s\n",
            "\n",
            "[main b9c9739] Add trained model weights\n",
            " 1 file changed, 0 insertions(+), 0 deletions(-)\n",
            "Enumerating objects: 9, done.\n",
            "Counting objects: 100% (9/9), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (3/3), done.\n",
            "Writing objects: 100% (5/5), 27.76 MiB | 8.85 MiB/s, done.\n",
            "Total 5 (delta 1), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n",
            "remote: \u001b[1;33mwarning\u001b[m: See https://gh.io/lfs for more information.\u001b[K\n",
            "remote: \u001b[1;33mwarning\u001b[m: File DDECCT_Results/BCH__Code_n_63_k_45/best_model is 51.57 MB; this is larger than GitHub's recommended maximum file size of 50.00 MB\u001b[K\n",
            "remote: \u001b[1;33mwarning\u001b[m: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.\u001b[K\n",
            "To https://github.com/pollyjuice74/DDECC.git\n",
            "   9f2ac9b..b9c9739  main -> main\n",
            "Training epoch 11, Batch 500/1000: LR=5.00e-04, Loss=1.17053e-03\n",
            "Training epoch 11, Batch 1000/1000: LR=5.00e-04, Loss=1.16140e-03\n",
            "Epoch 11 Train Time 34.776665925979614s\n",
            "\n",
            "Training epoch 12, Batch 500/1000: LR=5.00e-04, Loss=1.16794e-03\n",
            "Training epoch 12, Batch 1000/1000: LR=5.00e-04, Loss=1.15913e-03\n",
            "Epoch 12 Train Time 34.33795213699341s\n",
            "\n",
            "Training epoch 13, Batch 500/1000: LR=5.00e-04, Loss=1.18849e-03\n",
            "Training epoch 13, Batch 1000/1000: LR=5.00e-04, Loss=1.15817e-03\n",
            "Epoch 13 Train Time 33.84556436538696s\n",
            "\n",
            "Training epoch 14, Batch 500/1000: LR=5.00e-04, Loss=1.12550e-03\n",
            "Training epoch 14, Batch 1000/1000: LR=5.00e-04, Loss=1.16836e-03\n",
            "Epoch 14 Train Time 35.17593264579773s\n",
            "\n",
            "Training epoch 15, Batch 500/1000: LR=5.00e-04, Loss=1.07816e-03\n",
            "Training epoch 15, Batch 1000/1000: LR=5.00e-04, Loss=1.12397e-03\n",
            "Epoch 15 Train Time 34.90689277648926s\n",
            "\n",
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "Everything up-to-date\n",
            "Training epoch 16, Batch 500/1000: LR=5.00e-04, Loss=1.16316e-03\n",
            "Training epoch 16, Batch 1000/1000: LR=5.00e-04, Loss=1.15701e-03\n",
            "Epoch 16 Train Time 34.995981216430664s\n",
            "\n",
            "Training epoch 17, Batch 500/1000: LR=5.00e-04, Loss=1.18280e-03\n",
            "Training epoch 17, Batch 1000/1000: LR=5.00e-04, Loss=1.17081e-03\n",
            "Epoch 17 Train Time 34.55010628700256s\n",
            "\n",
            "Training epoch 18, Batch 500/1000: LR=5.00e-04, Loss=1.17173e-03\n",
            "Training epoch 18, Batch 1000/1000: LR=5.00e-04, Loss=1.15449e-03\n",
            "Epoch 18 Train Time 34.940688610076904s\n",
            "\n",
            "Training epoch 19, Batch 500/1000: LR=5.00e-04, Loss=1.21498e-03\n",
            "Training epoch 19, Batch 1000/1000: LR=5.00e-04, Loss=1.18370e-03\n",
            "Epoch 19 Train Time 35.45777082443237s\n",
            "\n",
            "Training epoch 20, Batch 500/1000: LR=5.00e-04, Loss=1.14248e-03\n",
            "Training epoch 20, Batch 1000/1000: LR=5.00e-04, Loss=1.14517e-03\n",
            "Epoch 20 Train Time 35.23443794250488s\n",
            "\n",
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "Everything up-to-date\n",
            "Training epoch 21, Batch 500/1000: LR=5.00e-04, Loss=1.23358e-03\n",
            "Training epoch 21, Batch 1000/1000: LR=5.00e-04, Loss=1.18628e-03\n",
            "Epoch 21 Train Time 34.882843255996704s\n",
            "\n",
            "Training epoch 22, Batch 500/1000: LR=5.00e-04, Loss=1.20460e-03\n",
            "Training epoch 22, Batch 1000/1000: LR=5.00e-04, Loss=1.20092e-03\n",
            "Epoch 22 Train Time 36.432610273361206s\n",
            "\n",
            "Training epoch 23, Batch 500/1000: LR=5.00e-04, Loss=1.16146e-03\n",
            "Training epoch 23, Batch 1000/1000: LR=5.00e-04, Loss=1.14627e-03\n",
            "Epoch 23 Train Time 36.10441708564758s\n",
            "\n",
            "Training epoch 24, Batch 500/1000: LR=5.00e-04, Loss=1.17121e-03\n",
            "Training epoch 24, Batch 1000/1000: LR=5.00e-04, Loss=1.18052e-03\n",
            "Epoch 24 Train Time 36.1670298576355s\n",
            "\n",
            "Training epoch 25, Batch 500/1000: LR=5.00e-04, Loss=1.21884e-03\n",
            "Training epoch 25, Batch 1000/1000: LR=5.00e-04, Loss=1.18137e-03\n",
            "Epoch 25 Train Time 35.88631010055542s\n",
            "\n",
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "Everything up-to-date\n",
            "Training epoch 26, Batch 500/1000: LR=5.00e-04, Loss=1.15170e-03\n",
            "Training epoch 26, Batch 1000/1000: LR=5.00e-04, Loss=1.14908e-03\n",
            "Epoch 26 Train Time 35.391324520111084s\n",
            "\n",
            "Training epoch 27, Batch 500/1000: LR=5.00e-04, Loss=1.20587e-03\n",
            "Training epoch 27, Batch 1000/1000: LR=5.00e-04, Loss=1.18920e-03\n",
            "Epoch 27 Train Time 35.485963582992554s\n",
            "\n",
            "Training epoch 28, Batch 500/1000: LR=5.00e-04, Loss=1.10725e-03\n",
            "Training epoch 28, Batch 1000/1000: LR=5.00e-04, Loss=1.10089e-03\n",
            "Epoch 28 Train Time 35.69531559944153s\n",
            "\n",
            "Model Saved\n",
            "Training epoch 29, Batch 500/1000: LR=5.00e-04, Loss=1.14609e-03\n",
            "Training epoch 29, Batch 1000/1000: LR=5.00e-04, Loss=1.11655e-03\n",
            "Epoch 29 Train Time 36.086482763290405s\n",
            "\n",
            "Training epoch 30, Batch 500/1000: LR=5.00e-04, Loss=1.14502e-03\n",
            "Training epoch 30, Batch 1000/1000: LR=5.00e-04, Loss=1.15485e-03\n",
            "Epoch 30 Train Time 36.50405287742615s\n",
            "\n",
            "[main 4999897] Add trained model weights\n",
            " 1 file changed, 0 insertions(+), 0 deletions(-)\n",
            "Enumerating objects: 9, done.\n",
            "Counting objects: 100% (9/9), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (3/3), done.\n",
            "Writing objects: 100% (5/5), 27.76 MiB | 7.74 MiB/s, done.\n",
            "Total 5 (delta 1), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n",
            "remote: \u001b[1;33mwarning\u001b[m: See https://gh.io/lfs for more information.\u001b[K\n",
            "remote: \u001b[1;33mwarning\u001b[m: File DDECCT_Results/BCH__Code_n_63_k_45/best_model is 51.57 MB; this is larger than GitHub's recommended maximum file size of 50.00 MB\u001b[K\n",
            "remote: \u001b[1;33mwarning\u001b[m: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.\u001b[K\n",
            "To https://github.com/pollyjuice74/DDECC.git\n",
            "   b9c9739..4999897  main -> main\n",
            "Training epoch 31, Batch 500/1000: LR=5.00e-04, Loss=1.17032e-03\n",
            "Training epoch 31, Batch 1000/1000: LR=5.00e-04, Loss=1.12065e-03\n",
            "Epoch 31 Train Time 36.46705102920532s\n",
            "\n",
            "Training epoch 32, Batch 500/1000: LR=5.00e-04, Loss=1.13492e-03\n",
            "Training epoch 32, Batch 1000/1000: LR=5.00e-04, Loss=1.10352e-03\n",
            "Epoch 32 Train Time 35.51282548904419s\n",
            "\n",
            "Training epoch 33, Batch 500/1000: LR=5.00e-04, Loss=1.08551e-03\n",
            "Training epoch 33, Batch 1000/1000: LR=5.00e-04, Loss=1.08841e-03\n",
            "Epoch 33 Train Time 36.18019080162048s\n",
            "\n",
            "Model Saved\n",
            "Training epoch 34, Batch 500/1000: LR=5.00e-04, Loss=1.13689e-03\n",
            "Training epoch 34, Batch 1000/1000: LR=5.00e-04, Loss=1.13113e-03\n",
            "Epoch 34 Train Time 36.36821484565735s\n",
            "\n",
            "Training epoch 35, Batch 500/1000: LR=5.00e-04, Loss=1.12216e-03\n",
            "Training epoch 35, Batch 1000/1000: LR=5.00e-04, Loss=1.14772e-03\n",
            "Epoch 35 Train Time 36.263692140579224s\n",
            "\n",
            "[main 50421b4] Add trained model weights\n",
            " 1 file changed, 0 insertions(+), 0 deletions(-)\n",
            "Enumerating objects: 9, done.\n",
            "Counting objects: 100% (9/9), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (3/3), done.\n",
            "Writing objects: 100% (5/5), 27.74 MiB | 7.54 MiB/s, done.\n",
            "Total 5 (delta 1), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n",
            "remote: \u001b[1;33mwarning\u001b[m: See https://gh.io/lfs for more information.\u001b[K\n",
            "remote: \u001b[1;33mwarning\u001b[m: File DDECCT_Results/BCH__Code_n_63_k_45/best_model is 51.57 MB; this is larger than GitHub's recommended maximum file size of 50.00 MB\u001b[K\n",
            "remote: \u001b[1;33mwarning\u001b[m: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.\u001b[K\n",
            "To https://github.com/pollyjuice74/DDECC.git\n",
            "   4999897..50421b4  main -> main\n",
            "Training epoch 36, Batch 500/1000: LR=5.00e-04, Loss=1.10928e-03\n",
            "Training epoch 36, Batch 1000/1000: LR=5.00e-04, Loss=1.12176e-03\n",
            "Epoch 36 Train Time 36.00806975364685s\n",
            "\n",
            "Training epoch 37, Batch 500/1000: LR=5.00e-04, Loss=1.13863e-03\n",
            "Training epoch 37, Batch 1000/1000: LR=5.00e-04, Loss=1.15527e-03\n",
            "Epoch 37 Train Time 36.020466566085815s\n",
            "\n",
            "Training epoch 38, Batch 500/1000: LR=5.00e-04, Loss=1.17842e-03\n",
            "Training epoch 38, Batch 1000/1000: LR=5.00e-04, Loss=1.15597e-03\n",
            "Epoch 38 Train Time 35.1792426109314s\n",
            "\n",
            "Training epoch 39, Batch 500/1000: LR=5.00e-04, Loss=1.12039e-03\n",
            "Training epoch 39, Batch 1000/1000: LR=5.00e-04, Loss=1.12998e-03\n",
            "Epoch 39 Train Time 35.382052421569824s\n",
            "\n",
            "Training epoch 40, Batch 500/1000: LR=5.00e-04, Loss=1.10783e-03\n",
            "Training epoch 40, Batch 1000/1000: LR=5.00e-04, Loss=1.10092e-03\n",
            "Epoch 40 Train Time 35.55743980407715s\n",
            "\n",
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "Everything up-to-date\n",
            "Training epoch 41, Batch 500/1000: LR=5.00e-04, Loss=1.15828e-03\n",
            "Training epoch 41, Batch 1000/1000: LR=5.00e-04, Loss=1.17854e-03\n",
            "Epoch 41 Train Time 35.34386420249939s\n",
            "\n",
            "Training epoch 42, Batch 500/1000: LR=4.99e-04, Loss=1.11271e-03\n",
            "Training epoch 42, Batch 1000/1000: LR=4.99e-04, Loss=1.10035e-03\n",
            "Epoch 42 Train Time 35.14072799682617s\n",
            "\n",
            "Training epoch 43, Batch 500/1000: LR=4.99e-04, Loss=1.18903e-03\n",
            "Training epoch 43, Batch 1000/1000: LR=4.99e-04, Loss=1.12691e-03\n",
            "Epoch 43 Train Time 34.75972390174866s\n",
            "\n",
            "Training epoch 44, Batch 500/1000: LR=4.99e-04, Loss=1.24433e-03\n",
            "Training epoch 44, Batch 1000/1000: LR=4.99e-04, Loss=1.17784e-03\n",
            "Epoch 44 Train Time 35.01878333091736s\n",
            "\n",
            "Training epoch 45, Batch 500/1000: LR=4.99e-04, Loss=1.10221e-03\n",
            "Training epoch 45, Batch 1000/1000: LR=4.99e-04, Loss=1.10293e-03\n",
            "Epoch 45 Train Time 35.23195147514343s\n",
            "\n",
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "Everything up-to-date\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-faca5bf208bd>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mbest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     loss= train(model, device, train_dataloader, optimizer,\n\u001b[0m\u001b[1;32m     39\u001b[0m                             epoch, LR=scheduler.get_last_lr()[0])\n\u001b[1;32m     40\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-e5ba0b3648ff>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch, LR)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mcum_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     for batch_idx, (m, x, z, y, magnitude, syndrome) in enumerate(\n\u001b[0m\u001b[1;32m     55\u001b[0m             train_loader):\n\u001b[1;32m     56\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbin_to_sign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1293\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrebuild_storage_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m     \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/reduction.py\u001b[0m in \u001b[0;36mrecv_handle\u001b[0;34m(conn)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;34m'''Receive a handle over a local connection.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAF_UNIX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mrecvfds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mDupFd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/reduction.py\u001b[0m in \u001b[0;36mrecvfds\u001b[0;34m(sock, size)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mbytes_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mancdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecvmsg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCMSG_SPACE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mancdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "code = args.code\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# MODEL #\n",
        "#################################\n",
        "model = DDECCT(args, device=device,dropout=0).to(device)\n",
        "model.ema.register(model)\n",
        "\n",
        "print('Loading Best Model')\n",
        "print(args.path)\n",
        "print(os.getcwd())\n",
        "model = torch.load(os.path.join(args.path, 'best_model')).to(device)\n",
        "#################################\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=args.epochs, eta_min=5e-6)\n",
        "\n",
        "print(model)\n",
        "print(f'# of Parameters: {np.sum([np.prod(p.shape) for p in model.parameters()])}')\n",
        "\n",
        "#################################\n",
        "EbNo_range_test = range(4, 7)\n",
        "EbNo_range_train = range(2, 8)\n",
        "std_train = [EbN0_to_std(ii, code.k / code.n) for ii in EbNo_range_train]\n",
        "std_test = [EbN0_to_std(ii, code.k / code.n) for ii in EbNo_range_test]\n",
        "train_dataloader = DataLoader(FEC_Dataset(code, std_train, len=args.batch_size * 1000, zero_cw=True), batch_size=int(args.batch_size),\n",
        "                              shuffle=True, num_workers=args.workers)\n",
        "test_dataloader_list = [DataLoader(FEC_Dataset(code, [std_test[ii]], len=int(args.test_batch_size), zero_cw=False),\n",
        "                                    batch_size=int(args.test_batch_size), shuffle=False, num_workers=args.workers) for ii in range(len(std_test))]\n",
        "#################################\n",
        "\n",
        "print(f\"Training model with code type: {args.code_type}\")\n",
        "print(args.code_type)\n",
        "\n",
        "\n",
        "best_loss = float('inf')\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    loss= train(model, device, train_dataloader, optimizer,\n",
        "                            epoch, LR=scheduler.get_last_lr()[0])\n",
        "    scheduler.step()\n",
        "    if loss < best_loss:\n",
        "        best_loss = loss\n",
        "        torch.save(model, os.path.join(args.path, 'best_model'))\n",
        "        print(f'Model Saved')\n",
        "    if epoch % 5 == 0 or epoch in [1,25]:\n",
        "\n",
        "        ### PUSH TO GITHUB ###\n",
        "        !git config --global user.name \"pollyjuice74\"\n",
        "        !git config --global user.email \"hernandez.aht82836@gmail.com\"\n",
        "\n",
        "        !git remote set-url origin https://pollyjuice74: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@github.com/pollyjuice74/DDECC.git\n",
        "\n",
        "        !git add .\n",
        "        !git commit -m \"Add trained model weights\"\n",
        "        !git push origin main\n",
        "        ######################\n",
        "\n",
        "        #test(model, device, test_dataloader_list, EbNo_range_test,min_FER=50,max_cum_count=1e6,min_cum_count=1e4)\n",
        "#################################\n",
        "\n",
        "print('Regular Reverse Diffusion')\n",
        "test(model, device, test_dataloader_list, EbNo_range_test,min_FER=100)\n",
        "print('Line Search Reverse Diffusion')\n",
        "model.line_search = True\n",
        "test(model, device, test_dataloader_list, EbNo_range_test,min_FER=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6dvw-yzGoWoW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d991a269-6cc3-4fdb-ae9c-c0e6fdd022eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "Everything up-to-date\n"
          ]
        }
      ],
      "source": [
        "!git config --global user.name \"pollyjuice74\"\n",
        "!git config --global user.email \"hernandez.aht82836@gmail.com\"\n",
        "\n",
        "!git remote set-url origin https://pollyjuice74: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@github.com/pollyjuice74/DDECC.git\n",
        "\n",
        "!git add .\n",
        "!git commit -m \"Add trained model weights\"\n",
        "!git push origin main"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eijLW5c8cSz7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP7QTDaHSS+Rx5VD8VhsSb8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}