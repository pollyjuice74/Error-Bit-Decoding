{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNq0YJoygzEpjdUI3hgGeP2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pollyjuice74/Error-Bit-Decoding/blob/main/GNNDecoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvwoXL0xJpoY",
        "outputId": "8cfa7db1-0528-41b4-eee9-a4b350982128"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sionna in /usr/local/lib/python3.10/dist-packages (0.16.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sionna) (1.23.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from sionna) (3.7.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sionna) (1.11.4)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from sionna) (6.1.1)\n",
            "Requirement already satisfied: mitsuba>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from sionna) (3.5.0)\n",
            "Requirement already satisfied: pythreejs>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from sionna) (2.4.2)\n",
            "Requirement already satisfied: ipywidgets>=8.0.4 in /usr/local/lib/python3.10/dist-packages (from sionna) (8.0.5)\n",
            "Requirement already satisfied: ipydatawidgets==4.3.2 in /usr/local/lib/python3.10/dist-packages (from sionna) (4.3.2)\n",
            "Requirement already satisfied: jupyterlab-widgets==3.0.5 in /usr/local/lib/python3.10/dist-packages (from sionna) (3.0.5)\n",
            "Requirement already satisfied: tensorflow!=2.11.0,<2.16.0,>=2.10.1 in /usr/local/lib/python3.10/dist-packages (from sionna) (2.15.0)\n",
            "Requirement already satisfied: traittypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipydatawidgets==4.3.2->sionna) (0.2.1)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->sionna) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->sionna) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->sionna) (4.0.9)\n",
            "Requirement already satisfied: drjit==0.4.4 in /usr/local/lib/python3.10/dist-packages (from mitsuba>=3.2.0->sionna) (0.4.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (0.35.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (2.15.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sionna) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sionna) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sionna) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sionna) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sionna) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sionna) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->sionna) (2.8.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (0.42.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (4.9.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (1.3.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->sionna) (0.2.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (2.1.4)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow!=2.11.0,<2.16.0,>=2.10.1->sionna) (3.2.2)\n",
            "fatal: destination path 'gnn-decoder' already exists and is not an empty directory.\n",
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# general imports\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# load required Sionna components\n",
        "!pip install sionna\n",
        "import sionna as sn\n",
        "from sionna.fec.utils import load_parity_check_examples, LinearEncoder, GaussianPriorSource\n",
        "from sionna.utils import BinarySource, ebnodb2no, BitwiseMutualInformation, hard_decisions\n",
        "from sionna.utils.metrics import compute_ber\n",
        "from sionna.utils.plotting import PlotBER\n",
        "from sionna.mapping import Mapper, Demapper\n",
        "from sionna.channel import AWGN\n",
        "from sionna.fec.ldpc import LDPCBPDecoder\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Layer\n",
        "\n",
        "!git clone https://github.com/NVlabs/gnn-decoder.git\n",
        "!\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import sys\n",
        "from importlib import import_module\n",
        "import pickle\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "#tf.config.experimental_run_functions_eagerly(True)\n",
        "#from gnn_decoder.gnn import *\n",
        "# gnn_decoder.wbp import * # load weighted BP functions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(Layer):\n",
        "    \"\"\"Simple MLP layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    units : List of int\n",
        "        Each element of the list describes the number of units of the\n",
        "        corresponding layer.\n",
        "\n",
        "    activations : List of activations\n",
        "        Each element of the list contains the activation to be used\n",
        "        by the corresponding layer.\n",
        "\n",
        "    use_bias : List of booleans\n",
        "        Each element of the list indicates if the corresponding layer\n",
        "        should use a bias or not.\n",
        "    \"\"\"\n",
        "    def __init__(self, units, activations, use_bias):\n",
        "        super().__init__()\n",
        "        self._num_units = units\n",
        "        self._activations = activations\n",
        "        self._use_bias = use_bias\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self._layers = []\n",
        "        for i, units in enumerate(self._num_units):\n",
        "            self._layers.append(Dense(units,\n",
        "                                      self._activations[i],\n",
        "                                      use_bias=self._use_bias[i]))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        outputs = inputs\n",
        "        for layer in self._layers:\n",
        "            outputs = layer(outputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class GNN_BP(Layer):\n",
        "    \"\"\"GNN-based BP Decoder\n",
        "\n",
        "    Parameters\n",
        "    ---------\n",
        "    H : [num_ch, num_vn], numpy.array\n",
        "        The parity check matrix.\n",
        "\n",
        "    num_embed_dims: int\n",
        "        Number of dimensions of the vertex embeddings.\n",
        "\n",
        "    num_msg_dims: int\n",
        "        Number of dimensions of a message.\n",
        "\n",
        "    num_hidden_units: int\n",
        "        Number of hidden units of the MLPs used to compute\n",
        "        messages and to update the vertex embeddings.\n",
        "\n",
        "    num_mlp_layers: int\n",
        "        Number of layers of the MLPs used to compute\n",
        "        messages and to update the vertex embeddings.\n",
        "\n",
        "    num_iter: int\n",
        "        Number of iterations.\n",
        "\n",
        "    reduce_op: str\n",
        "        A string defining the vertex aggregation function.\n",
        "        Currently, \"mean\" and \"sum\" is supported.\n",
        "\n",
        "    activation: str\n",
        "        A string defining the activation function of the hidden MLP layers to\n",
        "        be used. Defaults to \"relu\".\n",
        "\n",
        "    output_all_iter: Bool\n",
        "        Indicates if the LLRs of all iterations should be returned as list\n",
        "        or if only the LLRs of the last iteration should be returned.\n",
        "\n",
        "    clip_llr_to: float or None\n",
        "        If set, the absolute value of the input LLRs will be clipped to this value.\n",
        "\n",
        "    Input\n",
        "    -----\n",
        "    llr : [batch_size, num_vn], tf.float32\n",
        "        Tensor containing the LLRs of all bits.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "    llr_hat: : [batch_size, num_vn], tf.float32\n",
        "        Tensor containing the LLRs at the decoder output.\n",
        "        If `output_all_iter`==True, a list of such tensors will be returned.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 pcm,\n",
        "                 num_embed_dims,\n",
        "                 num_msg_dims,\n",
        "                 num_hidden_units,\n",
        "                 num_mlp_layers,\n",
        "                 num_iter,\n",
        "                 reduce_op=\"sum\",\n",
        "                 activation=\"relu\",\n",
        "                 output_all_iter=False,\n",
        "                 clip_llr_to=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self._pcm = pcm # Parity check matrix\n",
        "        self._num_cn = pcm.shape[0] # Number of check nodes\n",
        "        self._num_vn = pcm.shape[1] # Number of variables nodes\n",
        "        self._num_edges = int(np.sum(pcm)) # Number of edges\n",
        "\n",
        "        # Array of shape [num_edges, 2]\n",
        "        # 1st col = CN id, 2nd col = VN id\n",
        "        # The ith row of this array defines the ith edge.\n",
        "        self._edges = np.stack(np.where(pcm), axis=1)\n",
        "\n",
        "        # Create 2D ragged tensor of shape [num_cn,...]\n",
        "        # cn_edges[i] contains the edge ids for CN i\n",
        "        cn_edges = []\n",
        "        for i in range(self._num_cn):\n",
        "            cn_edges.append(np.where(self._edges[:,0]==i)[0])\n",
        "        self._cn_edges = tf.ragged.constant(cn_edges)\n",
        "\n",
        "        # Create 2D ragged tensor of shape [num_vn,...]\n",
        "        # vn_edges[i] contains the edge ids for VN i\n",
        "        vn_edges = []\n",
        "        for i in range(self._num_vn):\n",
        "            vn_edges.append(np.where(self._edges[:,1]==i)[0])\n",
        "        self._vn_edges = tf.ragged.constant(vn_edges)\n",
        "\n",
        "        self._num_embed_dims = num_embed_dims # Number of dimensions for vertex embeddings\n",
        "        self._num_msg_dims = num_msg_dims # Number of dimensions for messages\n",
        "        self._num_hidden_units = num_hidden_units # Number of hidden units for MLPs computing messages and embeddings\n",
        "        self._num_mlp_layers = num_mlp_layers # Number of layers for MLPs computing messages and embeddings\n",
        "        self._num_iter = num_iter # Number of BP iterations, can be modified\n",
        "\n",
        "        self._reduce_op = reduce_op # reduce operation for message aggregation\n",
        "        self._activation = activation # activation function of the hidden MLP layers\n",
        "\n",
        "        self._output_all_iter = output_all_iter\n",
        "        self._clip_llr_to = clip_llr_to\n",
        "\n",
        "    @property\n",
        "    def num_iter(self):\n",
        "        return self._num_iter\n",
        "\n",
        "    @num_iter.setter\n",
        "    def num_iter(self, value):\n",
        "        self._num_iter = value\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # NN to transform input LLR to VN embedding\n",
        "        self._llr_embed = Dense(self._num_embed_dims)\n",
        "\n",
        "        # NN to transform VN embedding to output LLR\n",
        "        self._llr_inv_embed = Dense(1)\n",
        "\n",
        "        # CN embedding update function\n",
        "        self.update_h_cn = UpdateEmbeddings(self._num_msg_dims,\n",
        "                                            self._num_hidden_units,\n",
        "                                            self._num_mlp_layers,\n",
        "                                            np.flip(self._edges, 1), # Flip columns: \"from VN to CN\"\n",
        "                                            self._cn_edges,\n",
        "                                            self._reduce_op,\n",
        "                                            self._activation)\n",
        "\n",
        "        # VN embedding update function\n",
        "        self.update_h_vn = UpdateEmbeddings(self._num_msg_dims,\n",
        "                                            self._num_hidden_units,\n",
        "                                            self._num_mlp_layers,\n",
        "                                            self._edges, # \"from CN to VN\"\n",
        "                                            self._vn_edges,\n",
        "                                            self._reduce_op,\n",
        "                                            self._activation)\n",
        "\n",
        "    def llr_to_embed(self, llr):\n",
        "        \"\"\"Transform LLRs to VN embeddings\"\"\"\n",
        "        return self._llr_embed(tf.expand_dims(llr, -1))\n",
        "\n",
        "    def embed_to_llr(self, h_vn):\n",
        "        \"\"\"Transform VN embeddings to LLRs\"\"\"\n",
        "        return tf.squeeze(self._llr_inv_embed(h_vn), axis=-1)\n",
        "\n",
        "    def call(self, llr):\n",
        "        batch_size = tf.shape(llr)[0]\n",
        "\n",
        "        # Initialize vertex embeddings\n",
        "        if self._clip_llr_to is not None:\n",
        "            llr = tf.clip_by_value(llr, -self._clip_llr_to, self._clip_llr_to)\n",
        "\n",
        "        h_vn = self.llr_to_embed(llr)\n",
        "        h_cn = tf.zeros([batch_size, self._num_cn, self._num_embed_dims])\n",
        "\n",
        "        # BP iterations\n",
        "        if self._output_all_iter:\n",
        "            llr_hat = []\n",
        "\n",
        "        for i in range(self._num_iter):\n",
        "            # Update CN embeddings\n",
        "            h_cn = self.update_h_cn(h_vn, h_cn)\n",
        "\n",
        "            # Update VNs\n",
        "            h_vn = self.update_h_vn(h_cn, h_vn)\n",
        "\n",
        "            if self._output_all_iter:\n",
        "                llr_hat.append(self.embed_to_llr(h_vn))\n",
        "\n",
        "        if not self._output_all_iter:\n",
        "            llr_hat = self.embed_to_llr(h_vn)\n",
        "\n",
        "        return llr_hat\n",
        "\n",
        "\n",
        "class UpdateEmbeddings(Layer):\n",
        "    \"\"\"Update vertex embeddings of the GNN BP decoder.\n",
        "\n",
        "    This layer computes first the messages that are sent across the edges\n",
        "    of the graph, then sums the incoming messages at each vertex, finally and\n",
        "    updates their embeddings.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    num_msg_dims: int\n",
        "        Number of dimensions of a message.\n",
        "\n",
        "    num_hidden_units: int\n",
        "        Number of hidden units of MLPs used to compute\n",
        "        messages and to update the vertex embeddings.\n",
        "\n",
        "    num_mlp_layers: int\n",
        "        Number of layers of the MLPs used to compute\n",
        "        messages and to update the vertex embeddings.\n",
        "\n",
        "    from_to_ind: [num_egdes, 2], np.array\n",
        "        Two dimensional array containing in each row the indices of the\n",
        "        originating and receiving vertex for an edge.\n",
        "\n",
        "    gather_ind: [`num_vn` or `num_cn`, None], tf.ragged.constant\n",
        "        Ragged tensor that contains for each receiving vertex the list of\n",
        "        edge indices from which to aggregate the incoming messages. As each\n",
        "        vertex can have a different degree, a ragged tensor is used.\n",
        "\n",
        "    reduce_op: str\n",
        "        A string defining the vertex aggregation function.\n",
        "        Currently, \"mean\" and \"sum\" is supported.\n",
        "\n",
        "    activation: str\n",
        "        A string defining the activation function of the hidden MLP layers to\n",
        "        be used. Defaults to \"relu\".\n",
        "\n",
        "    Input\n",
        "    -----\n",
        "    h_from : [batch_size, num_cn or num_vn, num_embed_dims], tf.float32\n",
        "        Tensor containing the embeddings of the \"transmitting\" vertices.\n",
        "\n",
        "    h_to : [batch_size, num_vn or num_cn, num_embed_dims], tf.float32\n",
        "        Tensor containing the embeddings of the \"receiving\" vertices.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "    h_to_new : Same shape and type as `h_to`\n",
        "        Tensor containing the updated embeddings of the \"receiving\" vertices.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 num_msg_dims,\n",
        "                 num_hidden_units,\n",
        "                 num_mlp_layers,\n",
        "                 from_to_ind,\n",
        "                 gather_ind,\n",
        "                 reduce_op=\"sum\",\n",
        "                 activation=\"relu\",\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self._num_msg_dims = num_msg_dims\n",
        "        self._num_hidden_units = num_hidden_units\n",
        "        self._num_mlp_layers = num_mlp_layers\n",
        "        self._from_ind = from_to_ind[:,0]\n",
        "        self._to_ind = from_to_ind[:,1]\n",
        "        self._gather_ind = gather_ind\n",
        "        self._reduce_op = reduce_op\n",
        "        self._activation = activation\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        num_embed_dims = input_shape[-1]\n",
        "\n",
        "        # MLP to compute messages\n",
        "        units = [self._num_hidden_units]*(self._num_mlp_layers-1) + [self._num_msg_dims]\n",
        "        activations = [self._activation]*(self._num_mlp_layers-1) + [None]\n",
        "        use_bias = [True]*self._num_mlp_layers\n",
        "        self._msg_mlp = MLP(units, activations, use_bias)\n",
        "\n",
        "        # MLP to update embeddings from accumulated messages\n",
        "        units[-1] = num_embed_dims\n",
        "        self._embed_mlp = MLP(units, activations, use_bias)\n",
        "\n",
        "    def call(self, h_from, h_to):\n",
        "        # Concatenate embeddings of the transmitting (from) and receiving (to) vertex for each edge\n",
        "        features = tf.concat([tf.gather(h_from, self._from_ind, axis=1),\n",
        "                              tf.gather(h_to, self._to_ind, axis=1)],\n",
        "                             axis=-1)\n",
        "\n",
        "        # Compute messsages for all edges\n",
        "        messages = self._msg_mlp(features)\n",
        "\n",
        "        # Reduce messages at each receiving (to) vertex\n",
        "        # note: bring batch dim to last dim for improved performance\n",
        "        # with ragged tensors\n",
        "        messages = tf.transpose(messages, (1,2,0))\n",
        "        m_ragged = tf.gather(messages, self._gather_ind, axis=0)\n",
        "        if self._reduce_op==\"sum\":\n",
        "            m = tf.reduce_sum(m_ragged, axis=1)\n",
        "        elif self._reduce_op==\"mean\":\n",
        "            m = tf.reduce_mean(m_ragged, axis=1)\n",
        "        else:\n",
        "            raise ValueError(\"unknown reduce operation\")\n",
        "        m = tf.transpose(m, (2,0,1)) # batch-dim back to first dim\n",
        "\n",
        "        # Compute new embeddings\n",
        "        h_to_new = self._embed_mlp(tf.concat([m, h_to], axis=-1))\n",
        "\n",
        "        return h_to_new\n",
        "\n",
        "\n",
        "\n",
        "class E2EModel(tf.keras.Model):\n",
        "    def __init__(self, pcm, decoder):\n",
        "        super().__init__()\n",
        "        self._pcm = pcm\n",
        "        self._n = pcm.shape[1]\n",
        "        self._k = self._n - pcm.shape[0]\n",
        "        self._encoder = LinearEncoder(pcm, is_pcm=True)\n",
        "\n",
        "        self._binary_source = BinarySource()\n",
        "\n",
        "        self._num_bits_per_symbol = 2 # at the moment only QPSK is supported\n",
        "        self._mapper = Mapper(\"qam\", self._num_bits_per_symbol)\n",
        "        self._demapper = Demapper(\"app\", \"qam\", self._num_bits_per_symbol)\n",
        "        self._channel = AWGN()\n",
        "        self._decoder = decoder\n",
        "\n",
        "    @tf.function()\n",
        "    def call(self, batch_size, ebno_db):\n",
        "\n",
        "        # calculate noise variance\n",
        "        if self._decoder is not None:\n",
        "            no = ebnodb2no(ebno_db, self._num_bits_per_symbol, self._k/self._n)\n",
        "        else: #for uncoded BPSK the rate is 1\n",
        "            no = ebnodb2no(ebno_db, self._num_bits_per_symbol, 1)\n",
        "\n",
        "        # draw random info bits to transmit\n",
        "        b = self._binary_source([batch_size, self._k])\n",
        "        c = self._encoder(b)\n",
        "\n",
        "        # zero padding to support odd codeword lengths\n",
        "        if self._n%2==1:\n",
        "            c_pad = tf.concat([c, tf.zeros([batch_size, 1])], axis=1)\n",
        "        else: # no padding\n",
        "            c_pad = c\n",
        "\n",
        "        # map to symbols\n",
        "        x = self._mapper(c_pad)\n",
        "\n",
        "        # transmit over AWGN channel\n",
        "        y = self._channel([x, no])\n",
        "\n",
        "        # demap to LLRs\n",
        "        llr = self._demapper([y, no])\n",
        "\n",
        "        # remove filler bits\n",
        "        if self._n%2==1:\n",
        "            llr = llr[:,:-1]\n",
        "\n",
        "        # and decode\n",
        "        if self._decoder is not None:\n",
        "            llr = self._decoder(llr)\n",
        "\n",
        "        return c, llr\n"
      ],
      "metadata": {
        "id": "4n_vRiorRtPm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, params):\n",
        "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "    for p in params:\n",
        "        train_batch_size, lr, train_iter = p\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "        @tf.function()\n",
        "        def train_step():\n",
        "            ebno_db = tf.random.uniform([train_batch_size, 1], minval=ebno_db_min, maxval=ebno_db_max)\n",
        "            with tf.GradientTape() as tape:\n",
        "                c, llr_hat = model(train_batch_size, ebno_db)\n",
        "                loss_value = 0\n",
        "                for m, l in enumerate(llr_hat):\n",
        "                    loss_value += loss(c, l)\n",
        "\n",
        "            weights = model.trainable_weights\n",
        "            grads = tape.gradient(loss_value, weights)\n",
        "            optimizer.apply_gradients(zip(grads, weights))\n",
        "            return c, llr_hat\n",
        "\n",
        "        for i in range(train_iter):\n",
        "            c, llr_hat = train_step()\n",
        "            if i%10==0:\n",
        "                ebno_db = tf.random.uniform([10000, 1],\n",
        "                                            minval=ebno_db_min,\n",
        "                                            maxval=ebno_db_max)\n",
        "                c, llr_hat = model(10000, ebno_db)\n",
        "                loss_value = 0\n",
        "                for l in llr_hat:\n",
        "                    loss_value += loss(c, l)\n",
        "                c_hat = tf.cast(tf.greater(llr_hat[-1], 0), tf.float32)\n",
        "                ber = compute_ber(c, c_hat).numpy()\n",
        "                print(f\"Iteration {i}, loss = {loss_value.numpy():.3f}, \" \\\n",
        "                      f\"ber = {ber:.5f}\")\n",
        "\n",
        "def load_weights(system, model_path):\n",
        "  \"\"\"Load model weights.\n",
        "\n",
        "  This function loads the weights of a Keras model ``system`` from a file\n",
        "  provided by ``model_path``.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "      system: Keras model\n",
        "          The target model into which the weights are loaded.\n",
        "\n",
        "      model_path: str\n",
        "          Defining the path where the weights are stored.\n",
        "\n",
        "  \"\"\"\n",
        "  with open(model_path, 'rb') as f:\n",
        "      weights = pickle.load(f)\n",
        "  system.set_weights(weights)"
      ],
      "metadata": {
        "id": "T547W9pHRPDD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BCH codes\n",
        "pcm, k, n, coderate = load_parity_check_examples(pcm_id=1, verbose=True)\n",
        "ebno_db_min = 2.0\n",
        "ebno_db_max = 9.0\n",
        "ebno_dbs = np.arange(ebno_db_min,ebno_db_max+1)\n",
        "\n",
        "mc_iters = 100\n",
        "mc_batch_size = 10000\n",
        "num_target_block_errors = 2000\n",
        "\n",
        "\n",
        "\n",
        "encoder = LinearEncoder(pcm, is_pcm=True)\n",
        "tf.random.set_seed(2)\n",
        "gnn_decoder = GNN_BP(pcm=pcm,\n",
        "                     num_embed_dims=20,\n",
        "                     num_msg_dims=20,\n",
        "                     num_hidden_units=40,\n",
        "                     num_mlp_layers=2,\n",
        "                     num_iter=8,\n",
        "                     reduce_op=\"mean\",\n",
        "                     activation=\"tanh\",\n",
        "                     output_all_iter=True,\n",
        "                     clip_llr_to=None)\n",
        "e2e_gnn = E2EModel(pcm, gnn_decoder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "396sLqYGMDv7",
        "outputId": "40f2e432-1e09-4c3c-9921-3560c73c0e81"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "n: 63, k: 45, coderate: 0.714\n",
            "Warning: The alias fec.utils.LinearEncoder will not be included in Sionna 1.0. Please use fec.linear.LinearEncoder instead.\n",
            "Warning: The alias fec.utils.LinearEncoder will not be included in Sionna 1.0. Please use fec.linear.LinearEncoder instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `model` is an instance of E2EModel\n",
        "e2e_gnn.save_weights('drive/MyDrive/weights/GNNDecoder.h5')\n"
      ],
      "metadata": {
        "id": "huoLVQGf4SXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e2e_gnn.load_weights('drive/MyDrive/weights/GNNDecoder.h5')\n",
        "train_params = [\n",
        "    #batch_size, learning_rate, num_iter\n",
        "    [256, 1e-3, 3500],\n",
        "    [256, 1e-4, 3000],\n",
        "    [256, 1e-5, 3000],\n",
        "]\n",
        "e2e_gnn._decoder._output_all_iter = True # use multi-loss during training\n",
        "train_model(e2e_gnn, train_params)\n",
        "\n",
        "#train = False#True #\n",
        "# if train:\n",
        "#     train_model(e2e_gnn, train_params)\n",
        "# else:\n",
        "#     # you can also load the precomputed weights\n",
        "#     load_weights(e2e_gnn, \"gnn_decoder/weights/LDPC_reg_precomputed.npy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrCTA8fJdea4",
        "outputId": "2a42322d-ccda-4e25-d751-35eb534c3a9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, loss = 0.324, ber = 0.01551\n",
            "Iteration 10, loss = 0.302, ber = 0.01268\n",
            "Iteration 20, loss = 0.278, ber = 0.01134\n",
            "Iteration 30, loss = 0.274, ber = 0.01105\n",
            "Iteration 40, loss = 0.281, ber = 0.01153\n",
            "Iteration 50, loss = 0.277, ber = 0.01127\n",
            "Iteration 60, loss = 0.270, ber = 0.01106\n",
            "Iteration 70, loss = 0.269, ber = 0.01115\n",
            "Iteration 80, loss = 0.274, ber = 0.01127\n",
            "Iteration 90, loss = 0.268, ber = 0.01083\n",
            "Iteration 100, loss = 0.267, ber = 0.01088\n",
            "Iteration 110, loss = 0.274, ber = 0.01121\n",
            "Iteration 120, loss = 0.262, ber = 0.01071\n",
            "Iteration 130, loss = 0.267, ber = 0.01090\n",
            "Iteration 140, loss = 0.257, ber = 0.01028\n",
            "Iteration 150, loss = 0.260, ber = 0.01053\n",
            "Iteration 160, loss = 0.267, ber = 0.01091\n",
            "Iteration 170, loss = 0.276, ber = 0.01112\n",
            "Iteration 180, loss = 0.267, ber = 0.01068\n",
            "Iteration 190, loss = 0.257, ber = 0.01049\n",
            "Iteration 200, loss = 0.272, ber = 0.01121\n",
            "Iteration 210, loss = 0.262, ber = 0.01066\n",
            "Iteration 220, loss = 0.277, ber = 0.01139\n",
            "Iteration 230, loss = 0.266, ber = 0.01057\n",
            "Iteration 240, loss = 0.268, ber = 0.01097\n",
            "Iteration 250, loss = 0.273, ber = 0.01111\n",
            "Iteration 260, loss = 0.274, ber = 0.01136\n",
            "Iteration 270, loss = 0.275, ber = 0.01130\n",
            "Iteration 280, loss = 0.271, ber = 0.01113\n",
            "Iteration 290, loss = 0.267, ber = 0.01076\n",
            "Iteration 300, loss = 0.268, ber = 0.01084\n",
            "Iteration 310, loss = 0.276, ber = 0.01132\n",
            "Iteration 320, loss = 0.275, ber = 0.01119\n",
            "Iteration 330, loss = 0.278, ber = 0.01147\n",
            "Iteration 340, loss = 0.274, ber = 0.01118\n",
            "Iteration 350, loss = 0.290, ber = 0.01170\n",
            "Iteration 360, loss = 0.276, ber = 0.01112\n",
            "Iteration 370, loss = 0.280, ber = 0.01161\n",
            "Iteration 380, loss = 0.281, ber = 0.01143\n",
            "Iteration 390, loss = 0.284, ber = 0.01163\n",
            "Iteration 400, loss = 0.274, ber = 0.01100\n",
            "Iteration 410, loss = 0.269, ber = 0.01084\n",
            "Iteration 420, loss = 0.266, ber = 0.01067\n",
            "Iteration 430, loss = 0.272, ber = 0.01120\n",
            "Iteration 440, loss = 0.275, ber = 0.01112\n",
            "Iteration 450, loss = 0.265, ber = 0.01077\n",
            "Iteration 460, loss = 0.270, ber = 0.01097\n",
            "Iteration 470, loss = 0.265, ber = 0.01081\n",
            "Iteration 480, loss = 0.273, ber = 0.01143\n",
            "Iteration 490, loss = 0.271, ber = 0.01087\n",
            "Iteration 500, loss = 0.268, ber = 0.01089\n",
            "Iteration 510, loss = 0.265, ber = 0.01073\n",
            "Iteration 520, loss = 0.278, ber = 0.01147\n",
            "Iteration 530, loss = 0.268, ber = 0.01100\n",
            "Iteration 540, loss = 0.275, ber = 0.01119\n",
            "Iteration 550, loss = 0.280, ber = 0.01152\n",
            "Iteration 560, loss = 0.268, ber = 0.01113\n",
            "Iteration 570, loss = 0.278, ber = 0.01136\n",
            "Iteration 580, loss = 0.269, ber = 0.01082\n",
            "Iteration 590, loss = 0.270, ber = 0.01089\n",
            "Iteration 600, loss = 0.272, ber = 0.01109\n",
            "Iteration 610, loss = 0.263, ber = 0.01060\n",
            "Iteration 620, loss = 0.276, ber = 0.01137\n",
            "Iteration 630, loss = 0.272, ber = 0.01121\n",
            "Iteration 640, loss = 0.268, ber = 0.01089\n",
            "Iteration 650, loss = 0.273, ber = 0.01131\n",
            "Iteration 660, loss = 0.267, ber = 0.01099\n",
            "Iteration 670, loss = 0.269, ber = 0.01103\n",
            "Iteration 680, loss = 0.269, ber = 0.01090\n",
            "Iteration 690, loss = 0.274, ber = 0.01115\n",
            "Iteration 700, loss = 0.284, ber = 0.01142\n",
            "Iteration 710, loss = 0.268, ber = 0.01078\n",
            "Iteration 720, loss = 0.263, ber = 0.01078\n",
            "Iteration 730, loss = 0.273, ber = 0.01110\n",
            "Iteration 740, loss = 0.267, ber = 0.01086\n",
            "Iteration 750, loss = 0.268, ber = 0.01084\n",
            "Iteration 760, loss = 0.270, ber = 0.01097\n",
            "Iteration 770, loss = 0.272, ber = 0.01107\n",
            "Iteration 780, loss = 0.275, ber = 0.01143\n",
            "Iteration 790, loss = 0.263, ber = 0.01059\n",
            "Iteration 800, loss = 0.274, ber = 0.01139\n",
            "Iteration 810, loss = 0.278, ber = 0.01144\n",
            "Iteration 820, loss = 0.269, ber = 0.01085\n",
            "Iteration 830, loss = 0.272, ber = 0.01137\n",
            "Iteration 840, loss = 0.274, ber = 0.01149\n",
            "Iteration 850, loss = 0.265, ber = 0.01094\n",
            "Iteration 860, loss = 0.267, ber = 0.01098\n",
            "Iteration 870, loss = 0.266, ber = 0.01103\n",
            "Iteration 880, loss = 0.275, ber = 0.01133\n",
            "Iteration 890, loss = 0.267, ber = 0.01087\n",
            "Iteration 900, loss = 0.261, ber = 0.01068\n",
            "Iteration 910, loss = 0.267, ber = 0.01077\n",
            "Iteration 920, loss = 0.265, ber = 0.01081\n",
            "Iteration 930, loss = 0.282, ber = 0.01153\n",
            "Iteration 940, loss = 0.268, ber = 0.01081\n",
            "Iteration 950, loss = 0.279, ber = 0.01141\n",
            "Iteration 960, loss = 0.289, ber = 0.01174\n",
            "Iteration 970, loss = 0.279, ber = 0.01133\n",
            "Iteration 980, loss = 0.267, ber = 0.01091\n",
            "Iteration 990, loss = 0.272, ber = 0.01134\n",
            "Iteration 1000, loss = 0.275, ber = 0.01121\n",
            "Iteration 1010, loss = 0.274, ber = 0.01153\n",
            "Iteration 1020, loss = 0.263, ber = 0.01076\n",
            "Iteration 1030, loss = 0.275, ber = 0.01123\n",
            "Iteration 1040, loss = 0.267, ber = 0.01052\n",
            "Iteration 1050, loss = 0.270, ber = 0.01092\n",
            "Iteration 1060, loss = 0.267, ber = 0.01092\n",
            "Iteration 1070, loss = 0.266, ber = 0.01081\n",
            "Iteration 1080, loss = 0.273, ber = 0.01119\n",
            "Iteration 1090, loss = 0.273, ber = 0.01105\n",
            "Iteration 1100, loss = 0.277, ber = 0.01124\n",
            "Iteration 1110, loss = 0.274, ber = 0.01114\n",
            "Iteration 1120, loss = 0.281, ber = 0.01151\n",
            "Iteration 1130, loss = 0.272, ber = 0.01122\n",
            "Iteration 1140, loss = 0.273, ber = 0.01116\n",
            "Iteration 1150, loss = 0.268, ber = 0.01107\n",
            "Iteration 1160, loss = 0.273, ber = 0.01084\n",
            "Iteration 1170, loss = 0.270, ber = 0.01126\n",
            "Iteration 1180, loss = 0.275, ber = 0.01119\n",
            "Iteration 1190, loss = 0.273, ber = 0.01104\n",
            "Iteration 1200, loss = 0.282, ber = 0.01157\n",
            "Iteration 1210, loss = 0.272, ber = 0.01096\n",
            "Iteration 1220, loss = 0.273, ber = 0.01096\n",
            "Iteration 1230, loss = 0.273, ber = 0.01097\n",
            "Iteration 1240, loss = 0.265, ber = 0.01092\n",
            "Iteration 1250, loss = 0.262, ber = 0.01062\n",
            "Iteration 1260, loss = 0.274, ber = 0.01135\n",
            "Iteration 1270, loss = 0.279, ber = 0.01130\n",
            "Iteration 1280, loss = 0.277, ber = 0.01149\n",
            "Iteration 1290, loss = 0.269, ber = 0.01091\n",
            "Iteration 1300, loss = 0.268, ber = 0.01086\n",
            "Iteration 1310, loss = 0.272, ber = 0.01102\n",
            "Iteration 1320, loss = 0.273, ber = 0.01118\n",
            "Iteration 1330, loss = 0.262, ber = 0.01066\n",
            "Iteration 1340, loss = 0.281, ber = 0.01160\n",
            "Iteration 1350, loss = 0.270, ber = 0.01107\n",
            "Iteration 1360, loss = 0.272, ber = 0.01130\n",
            "Iteration 1370, loss = 0.264, ber = 0.01087\n",
            "Iteration 1380, loss = 0.268, ber = 0.01088\n",
            "Iteration 1390, loss = 0.262, ber = 0.01065\n",
            "Iteration 1400, loss = 0.261, ber = 0.01058\n",
            "Iteration 1410, loss = 0.266, ber = 0.01094\n",
            "Iteration 1420, loss = 0.269, ber = 0.01108\n",
            "Iteration 1430, loss = 0.276, ber = 0.01133\n",
            "Iteration 1440, loss = 0.268, ber = 0.01092\n",
            "Iteration 1450, loss = 0.265, ber = 0.01085\n",
            "Iteration 1460, loss = 0.274, ber = 0.01115\n",
            "Iteration 1470, loss = 0.275, ber = 0.01116\n",
            "Iteration 1480, loss = 0.277, ber = 0.01151\n",
            "Iteration 1490, loss = 0.268, ber = 0.01097\n",
            "Iteration 1500, loss = 0.264, ber = 0.01060\n",
            "Iteration 1510, loss = 0.284, ber = 0.01162\n",
            "Iteration 1520, loss = 0.289, ber = 0.01201\n",
            "Iteration 1530, loss = 0.274, ber = 0.01119\n",
            "Iteration 1540, loss = 0.272, ber = 0.01128\n",
            "Iteration 1550, loss = 0.282, ber = 0.01163\n",
            "Iteration 1560, loss = 0.274, ber = 0.01101\n",
            "Iteration 1570, loss = 0.278, ber = 0.01129\n",
            "Iteration 1580, loss = 0.271, ber = 0.01134\n",
            "Iteration 1590, loss = 0.278, ber = 0.01186\n",
            "Iteration 1600, loss = 0.270, ber = 0.01096\n",
            "Iteration 1610, loss = 0.279, ber = 0.01149\n",
            "Iteration 1620, loss = 0.282, ber = 0.01165\n",
            "Iteration 1630, loss = 0.265, ber = 0.01075\n",
            "Iteration 1640, loss = 0.272, ber = 0.01096\n",
            "Iteration 1650, loss = 0.270, ber = 0.01088\n",
            "Iteration 1660, loss = 0.266, ber = 0.01090\n",
            "Iteration 1670, loss = 0.264, ber = 0.01064\n",
            "Iteration 1680, loss = 0.268, ber = 0.01104\n",
            "Iteration 1690, loss = 0.269, ber = 0.01121\n",
            "Iteration 1700, loss = 0.271, ber = 0.01113\n",
            "Iteration 1710, loss = 0.264, ber = 0.01088\n",
            "Iteration 1720, loss = 0.265, ber = 0.01094\n",
            "Iteration 1730, loss = 0.270, ber = 0.01104\n",
            "Iteration 1740, loss = 0.268, ber = 0.01082\n",
            "Iteration 1750, loss = 0.268, ber = 0.01085\n",
            "Iteration 1760, loss = 0.263, ber = 0.01076\n",
            "Iteration 1770, loss = 0.270, ber = 0.01102\n",
            "Iteration 1780, loss = 0.272, ber = 0.01114\n",
            "Iteration 1790, loss = 0.264, ber = 0.01057\n",
            "Iteration 1800, loss = 0.267, ber = 0.01081\n",
            "Iteration 1810, loss = 0.277, ber = 0.01147\n",
            "Iteration 1820, loss = 0.278, ber = 0.01120\n",
            "Iteration 1830, loss = 0.290, ber = 0.01160\n",
            "Iteration 1840, loss = 0.292, ber = 0.01169\n",
            "Iteration 1850, loss = 0.282, ber = 0.01147\n",
            "Iteration 1860, loss = 0.273, ber = 0.01125\n",
            "Iteration 1870, loss = 0.269, ber = 0.01091\n",
            "Iteration 1880, loss = 0.274, ber = 0.01114\n",
            "Iteration 1890, loss = 0.273, ber = 0.01113\n",
            "Iteration 1900, loss = 0.277, ber = 0.01172\n",
            "Iteration 1910, loss = 0.276, ber = 0.01136\n",
            "Iteration 1920, loss = 0.264, ber = 0.01052\n",
            "Iteration 1930, loss = 0.266, ber = 0.01085\n",
            "Iteration 1940, loss = 0.263, ber = 0.01043\n",
            "Iteration 1950, loss = 0.272, ber = 0.01106\n",
            "Iteration 1960, loss = 0.272, ber = 0.01118\n",
            "Iteration 1970, loss = 0.272, ber = 0.01126\n",
            "Iteration 1980, loss = 0.273, ber = 0.01098\n",
            "Iteration 1990, loss = 0.267, ber = 0.01079\n",
            "Iteration 2000, loss = 0.268, ber = 0.01102\n",
            "Iteration 2010, loss = 0.265, ber = 0.01085\n",
            "Iteration 2020, loss = 0.268, ber = 0.01074\n",
            "Iteration 2030, loss = 0.274, ber = 0.01115\n",
            "Iteration 2040, loss = 0.265, ber = 0.01080\n",
            "Iteration 2050, loss = 0.263, ber = 0.01048\n",
            "Iteration 2060, loss = 0.259, ber = 0.01042\n",
            "Iteration 2070, loss = 0.271, ber = 0.01114\n",
            "Iteration 2080, loss = 0.269, ber = 0.01098\n",
            "Iteration 2090, loss = 0.280, ber = 0.01144\n",
            "Iteration 2100, loss = 0.263, ber = 0.01062\n",
            "Iteration 2110, loss = 0.266, ber = 0.01061\n",
            "Iteration 2120, loss = 0.276, ber = 0.01146\n",
            "Iteration 2130, loss = 0.267, ber = 0.01091\n",
            "Iteration 2140, loss = 0.270, ber = 0.01089\n",
            "Iteration 2150, loss = 0.264, ber = 0.01065\n",
            "Iteration 2160, loss = 0.267, ber = 0.01075\n",
            "Iteration 2170, loss = 0.267, ber = 0.01086\n",
            "Iteration 2180, loss = 0.265, ber = 0.01088\n",
            "Iteration 2190, loss = 0.278, ber = 0.01125\n",
            "Iteration 2200, loss = 0.283, ber = 0.01176\n",
            "Iteration 2210, loss = 0.276, ber = 0.01112\n",
            "Iteration 2220, loss = 0.269, ber = 0.01108\n",
            "Iteration 2230, loss = 0.274, ber = 0.01116\n",
            "Iteration 2240, loss = 0.280, ber = 0.01161\n",
            "Iteration 2250, loss = 0.268, ber = 0.01087\n",
            "Iteration 2260, loss = 0.271, ber = 0.01106\n",
            "Iteration 2270, loss = 0.266, ber = 0.01081\n",
            "Iteration 2280, loss = 0.262, ber = 0.01064\n",
            "Iteration 2290, loss = 0.279, ber = 0.01145\n",
            "Iteration 2300, loss = 0.279, ber = 0.01147\n",
            "Iteration 2310, loss = 0.269, ber = 0.01099\n",
            "Iteration 2320, loss = 0.278, ber = 0.01176\n",
            "Iteration 2330, loss = 0.285, ber = 0.01160\n",
            "Iteration 2340, loss = 0.292, ber = 0.01221\n",
            "Iteration 2350, loss = 0.298, ber = 0.01258\n",
            "Iteration 2360, loss = 0.280, ber = 0.01171\n",
            "Iteration 2370, loss = 0.270, ber = 0.01093\n",
            "Iteration 2380, loss = 0.277, ber = 0.01153\n",
            "Iteration 2390, loss = 0.279, ber = 0.01158\n",
            "Iteration 2400, loss = 0.273, ber = 0.01124\n",
            "Iteration 2410, loss = 0.264, ber = 0.01082\n",
            "Iteration 2420, loss = 0.270, ber = 0.01122\n",
            "Iteration 2430, loss = 0.267, ber = 0.01086\n",
            "Iteration 2440, loss = 0.286, ber = 0.01180\n",
            "Iteration 2450, loss = 0.273, ber = 0.01110\n",
            "Iteration 2460, loss = 0.273, ber = 0.01131\n",
            "Iteration 2470, loss = 0.272, ber = 0.01131\n",
            "Iteration 2480, loss = 0.272, ber = 0.01100\n",
            "Iteration 2490, loss = 0.272, ber = 0.01103\n",
            "Iteration 2500, loss = 0.266, ber = 0.01091\n",
            "Iteration 2510, loss = 0.267, ber = 0.01111\n",
            "Iteration 2520, loss = 0.269, ber = 0.01115\n",
            "Iteration 2530, loss = 0.283, ber = 0.01181\n",
            "Iteration 2540, loss = 0.264, ber = 0.01052\n",
            "Iteration 2550, loss = 0.281, ber = 0.01166\n",
            "Iteration 2560, loss = 0.283, ber = 0.01209\n",
            "Iteration 2570, loss = 0.276, ber = 0.01165\n",
            "Iteration 2580, loss = 0.272, ber = 0.01107\n",
            "Iteration 2590, loss = 0.266, ber = 0.01086\n",
            "Iteration 2600, loss = 0.265, ber = 0.01070\n",
            "Iteration 2610, loss = 0.263, ber = 0.01080\n",
            "Iteration 2620, loss = 0.263, ber = 0.01067\n",
            "Iteration 2630, loss = 0.267, ber = 0.01088\n",
            "Iteration 2640, loss = 0.277, ber = 0.01124\n",
            "Iteration 2650, loss = 0.276, ber = 0.01132\n",
            "Iteration 2660, loss = 0.274, ber = 0.01145\n",
            "Iteration 2670, loss = 0.268, ber = 0.01087\n",
            "Iteration 2680, loss = 0.269, ber = 0.01095\n",
            "Iteration 2690, loss = 0.266, ber = 0.01122\n",
            "Iteration 2700, loss = 0.272, ber = 0.01107\n",
            "Iteration 2710, loss = 0.271, ber = 0.01119\n",
            "Iteration 2720, loss = 0.260, ber = 0.01066\n",
            "Iteration 2730, loss = 0.270, ber = 0.01104\n",
            "Iteration 2740, loss = 0.280, ber = 0.01152\n",
            "Iteration 2750, loss = 0.279, ber = 0.01191\n",
            "Iteration 2760, loss = 0.276, ber = 0.01147\n",
            "Iteration 2770, loss = 0.276, ber = 0.01117\n",
            "Iteration 2780, loss = 0.267, ber = 0.01097\n",
            "Iteration 2790, loss = 0.270, ber = 0.01094\n",
            "Iteration 2800, loss = 0.259, ber = 0.01043\n",
            "Iteration 2810, loss = 0.272, ber = 0.01109\n",
            "Iteration 2820, loss = 0.274, ber = 0.01122\n",
            "Iteration 2830, loss = 0.269, ber = 0.01102\n",
            "Iteration 2840, loss = 0.268, ber = 0.01101\n",
            "Iteration 2850, loss = 0.274, ber = 0.01126\n",
            "Iteration 2860, loss = 0.274, ber = 0.01128\n",
            "Iteration 2870, loss = 0.282, ber = 0.01163\n",
            "Iteration 2880, loss = 0.267, ber = 0.01089\n",
            "Iteration 2890, loss = 0.283, ber = 0.01148\n",
            "Iteration 2900, loss = 0.267, ber = 0.01072\n",
            "Iteration 2910, loss = 0.280, ber = 0.01137\n",
            "Iteration 2920, loss = 0.262, ber = 0.01068\n",
            "Iteration 2930, loss = 0.269, ber = 0.01095\n",
            "Iteration 2940, loss = 0.274, ber = 0.01102\n",
            "Iteration 2950, loss = 0.276, ber = 0.01156\n",
            "Iteration 2960, loss = 0.291, ber = 0.01207\n",
            "Iteration 2970, loss = 0.276, ber = 0.01167\n",
            "Iteration 2980, loss = 0.279, ber = 0.01134\n",
            "Iteration 2990, loss = 0.270, ber = 0.01135\n",
            "Iteration 3000, loss = 0.269, ber = 0.01083\n",
            "Iteration 3010, loss = 0.275, ber = 0.01126\n",
            "Iteration 3020, loss = 0.266, ber = 0.01057\n",
            "Iteration 3030, loss = 0.275, ber = 0.01129\n",
            "Iteration 3040, loss = 0.271, ber = 0.01149\n",
            "Iteration 3050, loss = 0.276, ber = 0.01133\n",
            "Iteration 3060, loss = 0.272, ber = 0.01114\n",
            "Iteration 3070, loss = 0.264, ber = 0.01079\n",
            "Iteration 3080, loss = 0.275, ber = 0.01123\n",
            "Iteration 3090, loss = 0.268, ber = 0.01095\n",
            "Iteration 3100, loss = 0.268, ber = 0.01090\n",
            "Iteration 3110, loss = 0.265, ber = 0.01091\n",
            "Iteration 3120, loss = 0.268, ber = 0.01090\n",
            "Iteration 3130, loss = 0.265, ber = 0.01066\n",
            "Iteration 3140, loss = 0.268, ber = 0.01076\n",
            "Iteration 3150, loss = 0.267, ber = 0.01094\n",
            "Iteration 3160, loss = 0.268, ber = 0.01081\n",
            "Iteration 3170, loss = 0.268, ber = 0.01109\n",
            "Iteration 3180, loss = 0.262, ber = 0.01086\n",
            "Iteration 3190, loss = 0.271, ber = 0.01124\n",
            "Iteration 3200, loss = 0.265, ber = 0.01072\n",
            "Iteration 3210, loss = 0.265, ber = 0.01078\n",
            "Iteration 3220, loss = 0.271, ber = 0.01109\n",
            "Iteration 3230, loss = 0.272, ber = 0.01123\n",
            "Iteration 3240, loss = 0.261, ber = 0.01058\n",
            "Iteration 3250, loss = 0.267, ber = 0.01114\n",
            "Iteration 3260, loss = 0.265, ber = 0.01090\n",
            "Iteration 3270, loss = 0.261, ber = 0.01050\n",
            "Iteration 3280, loss = 0.274, ber = 0.01143\n",
            "Iteration 3290, loss = 0.267, ber = 0.01093\n",
            "Iteration 3300, loss = 0.261, ber = 0.01061\n",
            "Iteration 3310, loss = 0.268, ber = 0.01087\n",
            "Iteration 3320, loss = 0.261, ber = 0.01057\n",
            "Iteration 3330, loss = 0.264, ber = 0.01074\n",
            "Iteration 3340, loss = 0.263, ber = 0.01074\n",
            "Iteration 3350, loss = 0.261, ber = 0.01064\n",
            "Iteration 3360, loss = 0.279, ber = 0.01131\n",
            "Iteration 3370, loss = 0.276, ber = 0.01113\n",
            "Iteration 3380, loss = 0.266, ber = 0.01101\n",
            "Iteration 3390, loss = 0.267, ber = 0.01102\n",
            "Iteration 3400, loss = 0.267, ber = 0.01106\n",
            "Iteration 3410, loss = 0.260, ber = 0.01055\n",
            "Iteration 3420, loss = 0.275, ber = 0.01140\n",
            "Iteration 3430, loss = 0.278, ber = 0.01127\n",
            "Iteration 3440, loss = 0.269, ber = 0.01100\n",
            "Iteration 3450, loss = 0.270, ber = 0.01106\n",
            "Iteration 3460, loss = 0.266, ber = 0.01082\n",
            "Iteration 3470, loss = 0.276, ber = 0.01133\n",
            "Iteration 3480, loss = 0.265, ber = 0.01044\n",
            "Iteration 3490, loss = 0.259, ber = 0.01039\n",
            "Iteration 0, loss = 0.269, ber = 0.01109\n",
            "Iteration 10, loss = 0.263, ber = 0.01083\n",
            "Iteration 20, loss = 0.265, ber = 0.01083\n",
            "Iteration 30, loss = 0.265, ber = 0.01086\n",
            "Iteration 40, loss = 0.266, ber = 0.01088\n",
            "Iteration 50, loss = 0.270, ber = 0.01103\n",
            "Iteration 60, loss = 0.266, ber = 0.01076\n",
            "Iteration 70, loss = 0.259, ber = 0.01044\n",
            "Iteration 80, loss = 0.260, ber = 0.01049\n",
            "Iteration 90, loss = 0.250, ber = 0.01006\n",
            "Iteration 100, loss = 0.260, ber = 0.01047\n",
            "Iteration 110, loss = 0.264, ber = 0.01074\n",
            "Iteration 120, loss = 0.265, ber = 0.01084\n",
            "Iteration 130, loss = 0.265, ber = 0.01079\n",
            "Iteration 140, loss = 0.261, ber = 0.01067\n",
            "Iteration 150, loss = 0.266, ber = 0.01079\n",
            "Iteration 160, loss = 0.256, ber = 0.01029\n",
            "Iteration 170, loss = 0.265, ber = 0.01093\n",
            "Iteration 180, loss = 0.263, ber = 0.01064\n",
            "Iteration 190, loss = 0.257, ber = 0.01061\n",
            "Iteration 200, loss = 0.260, ber = 0.01058\n",
            "Iteration 210, loss = 0.258, ber = 0.01053\n",
            "Iteration 220, loss = 0.258, ber = 0.01064\n",
            "Iteration 230, loss = 0.261, ber = 0.01072\n",
            "Iteration 240, loss = 0.267, ber = 0.01087\n",
            "Iteration 250, loss = 0.273, ber = 0.01113\n",
            "Iteration 260, loss = 0.264, ber = 0.01082\n",
            "Iteration 270, loss = 0.264, ber = 0.01076\n",
            "Iteration 280, loss = 0.263, ber = 0.01074\n",
            "Iteration 290, loss = 0.272, ber = 0.01113\n",
            "Iteration 300, loss = 0.270, ber = 0.01107\n",
            "Iteration 310, loss = 0.252, ber = 0.01005\n",
            "Iteration 320, loss = 0.269, ber = 0.01096\n",
            "Iteration 330, loss = 0.254, ber = 0.01041\n",
            "Iteration 340, loss = 0.259, ber = 0.01051\n",
            "Iteration 350, loss = 0.256, ber = 0.01031\n",
            "Iteration 360, loss = 0.272, ber = 0.01132\n",
            "Iteration 370, loss = 0.270, ber = 0.01101\n",
            "Iteration 380, loss = 0.253, ber = 0.01029\n",
            "Iteration 390, loss = 0.264, ber = 0.01089\n",
            "Iteration 400, loss = 0.265, ber = 0.01090\n",
            "Iteration 410, loss = 0.257, ber = 0.01038\n",
            "Iteration 420, loss = 0.269, ber = 0.01100\n",
            "Iteration 430, loss = 0.267, ber = 0.01095\n",
            "Iteration 440, loss = 0.261, ber = 0.01054\n",
            "Iteration 450, loss = 0.260, ber = 0.01057\n",
            "Iteration 460, loss = 0.266, ber = 0.01072\n",
            "Iteration 470, loss = 0.264, ber = 0.01077\n",
            "Iteration 480, loss = 0.264, ber = 0.01073\n",
            "Iteration 490, loss = 0.259, ber = 0.01052\n",
            "Iteration 500, loss = 0.257, ber = 0.01042\n",
            "Iteration 510, loss = 0.269, ber = 0.01101\n",
            "Iteration 520, loss = 0.263, ber = 0.01073\n",
            "Iteration 530, loss = 0.269, ber = 0.01100\n",
            "Iteration 540, loss = 0.265, ber = 0.01078\n",
            "Iteration 550, loss = 0.248, ber = 0.00996\n",
            "Iteration 560, loss = 0.257, ber = 0.01034\n",
            "Iteration 570, loss = 0.255, ber = 0.01034\n",
            "Iteration 580, loss = 0.263, ber = 0.01065\n",
            "Iteration 590, loss = 0.270, ber = 0.01122\n",
            "Iteration 600, loss = 0.253, ber = 0.01014\n",
            "Iteration 610, loss = 0.261, ber = 0.01080\n",
            "Iteration 620, loss = 0.264, ber = 0.01069\n",
            "Iteration 630, loss = 0.259, ber = 0.01046\n",
            "Iteration 640, loss = 0.258, ber = 0.01055\n",
            "Iteration 650, loss = 0.272, ber = 0.01101\n",
            "Iteration 660, loss = 0.260, ber = 0.01068\n",
            "Iteration 670, loss = 0.259, ber = 0.01053\n",
            "Iteration 680, loss = 0.262, ber = 0.01054\n",
            "Iteration 690, loss = 0.262, ber = 0.01064\n",
            "Iteration 700, loss = 0.258, ber = 0.01057\n",
            "Iteration 710, loss = 0.251, ber = 0.01009\n",
            "Iteration 720, loss = 0.264, ber = 0.01075\n",
            "Iteration 730, loss = 0.252, ber = 0.01012\n",
            "Iteration 740, loss = 0.270, ber = 0.01090\n",
            "Iteration 750, loss = 0.263, ber = 0.01067\n",
            "Iteration 760, loss = 0.263, ber = 0.01069\n",
            "Iteration 770, loss = 0.274, ber = 0.01123\n",
            "Iteration 780, loss = 0.261, ber = 0.01065\n",
            "Iteration 790, loss = 0.262, ber = 0.01066\n",
            "Iteration 800, loss = 0.263, ber = 0.01077\n",
            "Iteration 810, loss = 0.271, ber = 0.01112\n",
            "Iteration 820, loss = 0.257, ber = 0.01042\n",
            "Iteration 830, loss = 0.266, ber = 0.01088\n",
            "Iteration 840, loss = 0.254, ber = 0.01031\n",
            "Iteration 850, loss = 0.264, ber = 0.01068\n",
            "Iteration 860, loss = 0.267, ber = 0.01075\n",
            "Iteration 870, loss = 0.255, ber = 0.01021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ber_plot = PlotBER(\"GNN-based Decoding Results\")\n",
        "e2e_gnn._decoder._output_all_iter = False # deactivate multi-loss for inference\n",
        "ber_plot.simulate(e2e_gnn,\n",
        "                  ebno_dbs=ebno_dbs,\n",
        "                  batch_size=mc_batch_size,\n",
        "                  num_target_block_errors=num_target_block_errors,\n",
        "                  legend=\"GNN {} iter.\".format(gnn_decoder._num_iter),\n",
        "                  soft_estimates=True,\n",
        "                  max_mc_iter=mc_iters,\n",
        "                  forward_keyboard_interrupt=False,\n",
        "                  show_fig=False);\n",
        "\n",
        "\n",
        "e2e_uncoded = E2EModel(pcm, None)\n",
        "ber_plot.simulate(e2e_uncoded,\n",
        "                  ebno_dbs=ebno_dbs,\n",
        "                  batch_size=mc_batch_size,\n",
        "                  num_target_block_errors=num_target_block_errors,\n",
        "                  legend=\"Uncoded\",\n",
        "                  soft_estimates=True,\n",
        "                  max_mc_iter=mc_iters,\n",
        "                  forward_keyboard_interrupt=False,\n",
        "                  show_fig=False);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DX3kZaKfsAvw",
        "outputId": "6ec362ed-a6c2-4f86-b4c2-cd7d9472fa62"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EbNo [dB] |        BER |       BLER |  bit errors |    num bits | block errors |  num blocks | runtime [s] |    status\n",
            "---------------------------------------------------------------------------------------------------------------------------------------\n",
            "      2.0 | 6.7010e-02 | 9.6540e-01 |       42216 |      630000 |         9654 |       10000 |         1.2 |reached target block errors\n",
            "      3.0 | 4.1675e-02 | 8.1470e-01 |       26255 |      630000 |         8147 |       10000 |         1.2 |reached target block errors\n",
            "      4.0 | 2.0562e-02 | 5.3320e-01 |       12954 |      630000 |         5332 |       10000 |         1.2 |reached target block errors\n",
            "      5.0 | 6.9413e-03 | 2.2110e-01 |        4373 |      630000 |         2211 |       10000 |         1.2 |reached target block errors\n",
            "      6.0 | 1.7460e-03 | 6.9167e-02 |        3300 |     1890000 |         2075 |       30000 |         3.5 |reached target block errors\n",
            "      7.0 | 3.0148e-04 | 1.4047e-02 |        2849 |     9450000 |         2107 |      150000 |        17.3 |reached target block errors\n",
            "      8.0 | 3.8889e-05 | 2.1021e-03 |        2352 |    60480000 |         2018 |      960000 |       110.6 |reached target block errors\n",
            "      9.0 | 5.0794e-06 | 2.8500e-04 |         320 |    63000000 |          285 |     1000000 |       115.0 |reached max iter       \n",
            "Warning: The alias fec.utils.LinearEncoder will not be included in Sionna 1.0. Please use fec.linear.LinearEncoder instead.\n",
            "EbNo [dB] |        BER |       BLER |  bit errors |    num bits | block errors |  num blocks | runtime [s] |    status\n",
            "---------------------------------------------------------------------------------------------------------------------------------------\n",
            "      2.0 | 3.7433e-02 | 9.1480e-01 |       23583 |      630000 |         9148 |       10000 |         0.2 |reached target block errors\n",
            "      3.0 | 2.2829e-02 | 7.6690e-01 |       14382 |      630000 |         7669 |       10000 |         0.0 |reached target block errors\n",
            "      4.0 | 1.2676e-02 | 5.4700e-01 |        7986 |      630000 |         5470 |       10000 |         0.0 |reached target block errors\n",
            "      5.0 | 5.9667e-03 | 3.1730e-01 |        3759 |      630000 |         3173 |       10000 |         0.0 |reached target block errors\n",
            "      6.0 | 2.4198e-03 | 1.4150e-01 |        3049 |     1260000 |         2830 |       20000 |         0.0 |reached target block errors\n",
            "      7.0 | 7.6698e-04 | 4.7220e-02 |        2416 |     3150000 |         2361 |       50000 |         0.1 |reached target block errors\n",
            "      8.0 | 1.8777e-04 | 1.1776e-02 |        2011 |    10710000 |         2002 |      170000 |         0.4 |reached target block errors\n",
            "      9.0 | 3.3383e-05 | 2.1000e-03 |        2019 |    60480000 |         2016 |      960000 |         2.2 |reached target block errors\n"
          ]
        }
      ]
    }
  ]
}