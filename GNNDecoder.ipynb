{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO1nv363I6O4wFcsG08X9qk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pollyjuice74/Error-Bit-Decoding/blob/main/GNNDecoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvwoXL0xJpoY",
        "outputId": "1e2f2d74-446d-4388-9d85-886becfe36eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gnn-decoder'...\n",
            "remote: Enumerating objects: 46, done.\u001b[K\n",
            "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 46 (delta 22), reused 38 (delta 14), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (46/46), 857.22 KiB | 7.14 MiB/s, done.\n",
            "Resolving deltas: 100% (22/22), done.\n",
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "# general imports\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# load required Sionna components\n",
        "#!pip install sionna\n",
        "import sionna as sn\n",
        "from sionna.fec.utils import load_parity_check_examples, LinearEncoder, GaussianPriorSource\n",
        "from sionna.utils import BinarySource, ebnodb2no, BitwiseMutualInformation, hard_decisions\n",
        "from sionna.utils.metrics import compute_ber\n",
        "from sionna.utils.plotting import PlotBER\n",
        "from sionna.mapping import Mapper, Demapper\n",
        "from sionna.channel import AWGN\n",
        "from sionna.fec.ldpc import LDPCBPDecoder\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Layer\n",
        "\n",
        "!git clone https://github.com/NVlabs/gnn-decoder.git\n",
        "!\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import sys\n",
        "from importlib import import_module\n",
        "import pickle\n",
        "\n",
        "#tf.config.experimental_run_functions_eagerly(True)\n",
        "#from gnn_decoder.gnn import *\n",
        "# gnn_decoder.wbp import * # load weighted BP functions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(Layer):\n",
        "    \"\"\"Simple MLP layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    units : List of int\n",
        "        Each element of the list describes the number of units of the\n",
        "        corresponding layer.\n",
        "\n",
        "    activations : List of activations\n",
        "        Each element of the list contains the activation to be used\n",
        "        by the corresponding layer.\n",
        "\n",
        "    use_bias : List of booleans\n",
        "        Each element of the list indicates if the corresponding layer\n",
        "        should use a bias or not.\n",
        "    \"\"\"\n",
        "    def __init__(self, units, activations, use_bias):\n",
        "        super().__init__()\n",
        "        self._num_units = units\n",
        "        self._activations = activations\n",
        "        self._use_bias = use_bias\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self._layers = []\n",
        "        for i, units in enumerate(self._num_units):\n",
        "            self._layers.append(Dense(units,\n",
        "                                      self._activations[i],\n",
        "                                      use_bias=self._use_bias[i]))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        outputs = inputs\n",
        "        for layer in self._layers:\n",
        "            outputs = layer(outputs)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class GNN_BP(Layer):\n",
        "    \"\"\"GNN-based BP Decoder\n",
        "\n",
        "    Parameters\n",
        "    ---------\n",
        "    H : [num_ch, num_vn], numpy.array\n",
        "        The parity check matrix.\n",
        "\n",
        "    num_embed_dims: int\n",
        "        Number of dimensions of the vertex embeddings.\n",
        "\n",
        "    num_msg_dims: int\n",
        "        Number of dimensions of a message.\n",
        "\n",
        "    num_hidden_units: int\n",
        "        Number of hidden units of the MLPs used to compute\n",
        "        messages and to update the vertex embeddings.\n",
        "\n",
        "    num_mlp_layers: int\n",
        "        Number of layers of the MLPs used to compute\n",
        "        messages and to update the vertex embeddings.\n",
        "\n",
        "    num_iter: int\n",
        "        Number of iterations.\n",
        "\n",
        "    reduce_op: str\n",
        "        A string defining the vertex aggregation function.\n",
        "        Currently, \"mean\" and \"sum\" is supported.\n",
        "\n",
        "    activation: str\n",
        "        A string defining the activation function of the hidden MLP layers to\n",
        "        be used. Defaults to \"relu\".\n",
        "\n",
        "    output_all_iter: Bool\n",
        "        Indicates if the LLRs of all iterations should be returned as list\n",
        "        or if only the LLRs of the last iteration should be returned.\n",
        "\n",
        "    clip_llr_to: float or None\n",
        "        If set, the absolute value of the input LLRs will be clipped to this value.\n",
        "\n",
        "    Input\n",
        "    -----\n",
        "    llr : [batch_size, num_vn], tf.float32\n",
        "        Tensor containing the LLRs of all bits.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "    llr_hat: : [batch_size, num_vn], tf.float32\n",
        "        Tensor containing the LLRs at the decoder output.\n",
        "        If `output_all_iter`==True, a list of such tensors will be returned.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 pcm,\n",
        "                 num_embed_dims,\n",
        "                 num_msg_dims,\n",
        "                 num_hidden_units,\n",
        "                 num_mlp_layers,\n",
        "                 num_iter,\n",
        "                 reduce_op=\"sum\",\n",
        "                 activation=\"relu\",\n",
        "                 output_all_iter=False,\n",
        "                 clip_llr_to=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self._pcm = pcm # Parity check matrix\n",
        "        self._num_cn = pcm.shape[0] # Number of check nodes\n",
        "        self._num_vn = pcm.shape[1] # Number of variables nodes\n",
        "        self._num_edges = int(np.sum(pcm)) # Number of edges\n",
        "\n",
        "        # Array of shape [num_edges, 2]\n",
        "        # 1st col = CN id, 2nd col = VN id\n",
        "        # The ith row of this array defines the ith edge.\n",
        "        self._edges = np.stack(np.where(pcm), axis=1)\n",
        "\n",
        "        # Create 2D ragged tensor of shape [num_cn,...]\n",
        "        # cn_edges[i] contains the edge ids for CN i\n",
        "        cn_edges = []\n",
        "        for i in range(self._num_cn):\n",
        "            cn_edges.append(np.where(self._edges[:,0]==i)[0])\n",
        "        self._cn_edges = tf.ragged.constant(cn_edges)\n",
        "\n",
        "        # Create 2D ragged tensor of shape [num_vn,...]\n",
        "        # vn_edges[i] contains the edge ids for VN i\n",
        "        vn_edges = []\n",
        "        for i in range(self._num_vn):\n",
        "            vn_edges.append(np.where(self._edges[:,1]==i)[0])\n",
        "        self._vn_edges = tf.ragged.constant(vn_edges)\n",
        "\n",
        "        self._num_embed_dims = num_embed_dims # Number of dimensions for vertex embeddings\n",
        "        self._num_msg_dims = num_msg_dims # Number of dimensions for messages\n",
        "        self._num_hidden_units = num_hidden_units # Number of hidden units for MLPs computing messages and embeddings\n",
        "        self._num_mlp_layers = num_mlp_layers # Number of layers for MLPs computing messages and embeddings\n",
        "        self._num_iter = num_iter # Number of BP iterations, can be modified\n",
        "\n",
        "        self._reduce_op = reduce_op # reduce operation for message aggregation\n",
        "        self._activation = activation # activation function of the hidden MLP layers\n",
        "\n",
        "        self._output_all_iter = output_all_iter\n",
        "        self._clip_llr_to = clip_llr_to\n",
        "\n",
        "    @property\n",
        "    def num_iter(self):\n",
        "        return self._num_iter\n",
        "\n",
        "    @num_iter.setter\n",
        "    def num_iter(self, value):\n",
        "        self._num_iter = value\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # NN to transform input LLR to VN embedding\n",
        "        self._llr_embed = Dense(self._num_embed_dims)\n",
        "\n",
        "        # NN to transform VN embedding to output LLR\n",
        "        self._llr_inv_embed = Dense(1)\n",
        "\n",
        "        # CN embedding update function\n",
        "        self.update_h_cn = UpdateEmbeddings(self._num_msg_dims,\n",
        "                                            self._num_hidden_units,\n",
        "                                            self._num_mlp_layers,\n",
        "                                            np.flip(self._edges, 1), # Flip columns: \"from VN to CN\"\n",
        "                                            self._cn_edges,\n",
        "                                            self._reduce_op,\n",
        "                                            self._activation)\n",
        "\n",
        "        # VN embedding update function\n",
        "        self.update_h_vn = UpdateEmbeddings(self._num_msg_dims,\n",
        "                                            self._num_hidden_units,\n",
        "                                            self._num_mlp_layers,\n",
        "                                            self._edges, # \"from CN to VN\"\n",
        "                                            self._vn_edges,\n",
        "                                            self._reduce_op,\n",
        "                                            self._activation)\n",
        "\n",
        "    def llr_to_embed(self, llr):\n",
        "        \"\"\"Transform LLRs to VN embeddings\"\"\"\n",
        "        return self._llr_embed(tf.expand_dims(llr, -1))\n",
        "\n",
        "    def embed_to_llr(self, h_vn):\n",
        "        \"\"\"Transform VN embeddings to LLRs\"\"\"\n",
        "        return tf.squeeze(self._llr_inv_embed(h_vn), axis=-1)\n",
        "\n",
        "    def call(self, llr):\n",
        "        batch_size = tf.shape(llr)[0]\n",
        "\n",
        "        # Initialize vertex embeddings\n",
        "        if self._clip_llr_to is not None:\n",
        "            llr = tf.clip_by_value(llr, -self._clip_llr_to, self._clip_llr_to)\n",
        "\n",
        "        h_vn = self.llr_to_embed(llr)\n",
        "        h_cn = tf.zeros([batch_size, self._num_cn, self._num_embed_dims])\n",
        "\n",
        "        # BP iterations\n",
        "        if self._output_all_iter:\n",
        "            llr_hat = []\n",
        "\n",
        "        for i in range(self._num_iter):\n",
        "            # Update CN embeddings\n",
        "            h_cn = self.update_h_cn(h_vn, h_cn)\n",
        "\n",
        "            # Update VNs\n",
        "            h_vn = self.update_h_vn(h_cn, h_vn)\n",
        "\n",
        "            if self._output_all_iter:\n",
        "                llr_hat.append(self.embed_to_llr(h_vn))\n",
        "\n",
        "        if not self._output_all_iter:\n",
        "            llr_hat = self.embed_to_llr(h_vn)\n",
        "\n",
        "        return llr_hat\n",
        "\n",
        "\n",
        "class UpdateEmbeddings(Layer):\n",
        "    \"\"\"Update vertex embeddings of the GNN BP decoder.\n",
        "\n",
        "    This layer computes first the messages that are sent across the edges\n",
        "    of the graph, then sums the incoming messages at each vertex, finally and\n",
        "    updates their embeddings.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    num_msg_dims: int\n",
        "        Number of dimensions of a message.\n",
        "\n",
        "    num_hidden_units: int\n",
        "        Number of hidden units of MLPs used to compute\n",
        "        messages and to update the vertex embeddings.\n",
        "\n",
        "    num_mlp_layers: int\n",
        "        Number of layers of the MLPs used to compute\n",
        "        messages and to update the vertex embeddings.\n",
        "\n",
        "    from_to_ind: [num_egdes, 2], np.array\n",
        "        Two dimensional array containing in each row the indices of the\n",
        "        originating and receiving vertex for an edge.\n",
        "\n",
        "    gather_ind: [`num_vn` or `num_cn`, None], tf.ragged.constant\n",
        "        Ragged tensor that contains for each receiving vertex the list of\n",
        "        edge indices from which to aggregate the incoming messages. As each\n",
        "        vertex can have a different degree, a ragged tensor is used.\n",
        "\n",
        "    reduce_op: str\n",
        "        A string defining the vertex aggregation function.\n",
        "        Currently, \"mean\" and \"sum\" is supported.\n",
        "\n",
        "    activation: str\n",
        "        A string defining the activation function of the hidden MLP layers to\n",
        "        be used. Defaults to \"relu\".\n",
        "\n",
        "    Input\n",
        "    -----\n",
        "    h_from : [batch_size, num_cn or num_vn, num_embed_dims], tf.float32\n",
        "        Tensor containing the embeddings of the \"transmitting\" vertices.\n",
        "\n",
        "    h_to : [batch_size, num_vn or num_cn, num_embed_dims], tf.float32\n",
        "        Tensor containing the embeddings of the \"receiving\" vertices.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "    h_to_new : Same shape and type as `h_to`\n",
        "        Tensor containing the updated embeddings of the \"receiving\" vertices.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 num_msg_dims,\n",
        "                 num_hidden_units,\n",
        "                 num_mlp_layers,\n",
        "                 from_to_ind,\n",
        "                 gather_ind,\n",
        "                 reduce_op=\"sum\",\n",
        "                 activation=\"relu\",\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self._num_msg_dims = num_msg_dims\n",
        "        self._num_hidden_units = num_hidden_units\n",
        "        self._num_mlp_layers = num_mlp_layers\n",
        "        self._from_ind = from_to_ind[:,0]\n",
        "        self._to_ind = from_to_ind[:,1]\n",
        "        self._gather_ind = gather_ind\n",
        "        self._reduce_op = reduce_op\n",
        "        self._activation = activation\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        num_embed_dims = input_shape[-1]\n",
        "\n",
        "        # MLP to compute messages\n",
        "        units = [self._num_hidden_units]*(self._num_mlp_layers-1) + [self._num_msg_dims]\n",
        "        activations = [self._activation]*(self._num_mlp_layers-1) + [None]\n",
        "        use_bias = [True]*self._num_mlp_layers\n",
        "        self._msg_mlp = MLP(units, activations, use_bias)\n",
        "\n",
        "        # MLP to update embeddings from accumulated messages\n",
        "        units[-1] = num_embed_dims\n",
        "        self._embed_mlp = MLP(units, activations, use_bias)\n",
        "\n",
        "    def call(self, h_from, h_to):\n",
        "        # Concatenate embeddings of the transmitting (from) and receiving (to) vertex for each edge\n",
        "        features = tf.concat([tf.gather(h_from, self._from_ind, axis=1),\n",
        "                              tf.gather(h_to, self._to_ind, axis=1)],\n",
        "                             axis=-1)\n",
        "\n",
        "        # Compute messsages for all edges\n",
        "        messages = self._msg_mlp(features)\n",
        "\n",
        "        # Reduce messages at each receiving (to) vertex\n",
        "        # note: bring batch dim to last dim for improved performance\n",
        "        # with ragged tensors\n",
        "        messages = tf.transpose(messages, (1,2,0))\n",
        "        m_ragged = tf.gather(messages, self._gather_ind, axis=0)\n",
        "        if self._reduce_op==\"sum\":\n",
        "            m = tf.reduce_sum(m_ragged, axis=1)\n",
        "        elif self._reduce_op==\"mean\":\n",
        "            m = tf.reduce_mean(m_ragged, axis=1)\n",
        "        else:\n",
        "            raise ValueError(\"unknown reduce operation\")\n",
        "        m = tf.transpose(m, (2,0,1)) # batch-dim back to first dim\n",
        "\n",
        "        # Compute new embeddings\n",
        "        h_to_new = self._embed_mlp(tf.concat([m, h_to], axis=-1))\n",
        "\n",
        "        return h_to_new\n",
        "\n",
        "\n",
        "\n",
        "class E2EModel(tf.keras.Model):\n",
        "    def __init__(self, pcm, decoder):\n",
        "        super().__init__()\n",
        "        self._pcm = pcm\n",
        "        self._n = pcm.shape[1]\n",
        "        self._k = self._n - pcm.shape[0]\n",
        "        self._encoder = LinearEncoder(pcm, is_pcm=True)\n",
        "\n",
        "        self._binary_source = BinarySource()\n",
        "\n",
        "        self._num_bits_per_symbol = 2 # at the moment only QPSK is supported\n",
        "        self._mapper = Mapper(\"qam\", self._num_bits_per_symbol)\n",
        "        self._demapper = Demapper(\"app\", \"qam\", self._num_bits_per_symbol)\n",
        "        self._channel = AWGN()\n",
        "        self._decoder = decoder\n",
        "\n",
        "    @tf.function()\n",
        "    def call(self, batch_size, ebno_db):\n",
        "\n",
        "        # calculate noise variance\n",
        "        if self._decoder is not None:\n",
        "            no = ebnodb2no(ebno_db, self._num_bits_per_symbol, self._k/self._n)\n",
        "        else: #for uncoded BPSK the rate is 1\n",
        "            no = ebnodb2no(ebno_db, self._num_bits_per_symbol, 1)\n",
        "\n",
        "        # draw random info bits to transmit\n",
        "        b = self._binary_source([batch_size, self._k])\n",
        "        c = self._encoder(b)\n",
        "\n",
        "        # zero padding to support odd codeword lengths\n",
        "        if self._n%2==1:\n",
        "            c_pad = tf.concat([c, tf.zeros([batch_size, 1])], axis=1)\n",
        "        else: # no padding\n",
        "            c_pad = c\n",
        "\n",
        "        # map to symbols\n",
        "        x = self._mapper(c_pad)\n",
        "\n",
        "        # transmit over AWGN channel\n",
        "        y = self._channel([x, no])\n",
        "\n",
        "        # demap to LLRs\n",
        "        llr = self._demapper([y, no])\n",
        "\n",
        "        # remove filler bits\n",
        "        if self._n%2==1:\n",
        "            llr = llr[:,:-1]\n",
        "\n",
        "        # and decode\n",
        "        if self._decoder is not None:\n",
        "            llr = self._decoder(llr)\n",
        "\n",
        "        return c, llr\n"
      ],
      "metadata": {
        "id": "4n_vRiorRtPm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, params):\n",
        "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "    for p in params:\n",
        "        train_batch_size, lr, train_iter = p\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "        @tf.function()\n",
        "        def train_step():\n",
        "            ebno_db = tf.random.uniform([train_batch_size, 1], minval=ebno_db_min, maxval=ebno_db_max)\n",
        "            with tf.GradientTape() as tape:\n",
        "                c, llr_hat = model(train_batch_size, ebno_db)\n",
        "                loss_value = 0\n",
        "                for m, l in enumerate(llr_hat):\n",
        "                    loss_value += loss(c, l)\n",
        "\n",
        "            weights = model.trainable_weights\n",
        "            grads = tape.gradient(loss_value, weights)\n",
        "            optimizer.apply_gradients(zip(grads, weights))\n",
        "            return c, llr_hat\n",
        "\n",
        "        for i in range(train_iter):\n",
        "            c, llr_hat = train_step()\n",
        "            if i%10==0:\n",
        "                ebno_db = tf.random.uniform([10000, 1],\n",
        "                                            minval=ebno_db_min,\n",
        "                                            maxval=ebno_db_max)\n",
        "                c, llr_hat = model(10000, ebno_db)\n",
        "                loss_value = 0\n",
        "                for l in llr_hat:\n",
        "                    loss_value += loss(c, l)\n",
        "                c_hat = tf.cast(tf.greater(llr_hat[-1], 0), tf.float32)\n",
        "                ber = compute_ber(c, c_hat).numpy()\n",
        "                print(f\"Iteration {i}, loss = {loss_value.numpy():.3f}, \" \\\n",
        "                      f\"ber = {ber:.5f}\")\n",
        "\n",
        "def load_weights(system, model_path):\n",
        "  \"\"\"Load model weights.\n",
        "\n",
        "  This function loads the weights of a Keras model ``system`` from a file\n",
        "  provided by ``model_path``.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "      system: Keras model\n",
        "          The target model into which the weights are loaded.\n",
        "\n",
        "      model_path: str\n",
        "          Defining the path where the weights are stored.\n",
        "\n",
        "  \"\"\"\n",
        "  with open(model_path, 'rb') as f:\n",
        "      weights = pickle.load(f)\n",
        "  system.set_weights(weights)"
      ],
      "metadata": {
        "id": "T547W9pHRPDD"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BCH codes\n",
        "pcm, k, n, coderate = load_parity_check_examples(pcm_id=1, verbose=True)\n",
        "ebno_db_min = 2.0\n",
        "ebno_db_max = 9.0\n",
        "ebno_dbs = np.arange(ebno_db_min,ebno_db_max+1)\n",
        "\n",
        "mc_iters = 100\n",
        "mc_batch_size = 10000\n",
        "num_target_block_errors = 2000\n",
        "\n",
        "\n",
        "\n",
        "encoder = LinearEncoder(pcm, is_pcm=True)\n",
        "tf.random.set_seed(2)\n",
        "gnn_decoder = GNN_BP(pcm=pcm,\n",
        "                     num_embed_dims=20,\n",
        "                     num_msg_dims=20,\n",
        "                     num_hidden_units=40,\n",
        "                     num_mlp_layers=2,\n",
        "                     num_iter=8,\n",
        "                     reduce_op=\"mean\",\n",
        "                     activation=\"tanh\",\n",
        "                     output_all_iter=True,\n",
        "                     clip_llr_to=None)\n",
        "e2e_gnn = E2EModel(pcm, gnn_decoder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "396sLqYGMDv7",
        "outputId": "fee627a8-f62d-4b9a-90ca-e976a5da5990"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "n: 63, k: 45, coderate: 0.714\n",
            "Warning: The alias fec.utils.LinearEncoder will not be included in Sionna 1.0. Please use fec.linear.LinearEncoder instead.\n",
            "Warning: The alias fec.utils.LinearEncoder will not be included in Sionna 1.0. Please use fec.linear.LinearEncoder instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_params = [\n",
        "    #batch_size, learning_rate, num_iter\n",
        "    [512, 1e-3, 35000],\n",
        "    [512, 1e-4, 300000],\n",
        "    [512, 1e-5, 300000],\n",
        "]\n",
        "e2e_gnn._decoder._output_all_iter = True # use multi-loss during training\n",
        "train_model(e2e_gnn, train_params)\n",
        "\n",
        "\n",
        "#train = False#True #\n",
        "# if train:\n",
        "#     train_model(e2e_gnn, train_params)\n",
        "# else:\n",
        "#     # you can also load the precomputed weights\n",
        "#     load_weights(e2e_gnn, \"gnn_decoder/weights/LDPC_reg_precomputed.npy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrCTA8fJdea4",
        "outputId": "10be0ef6-54d3-43e5-9dd6-ed319d6582c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, loss = 0.575, ber = 0.03809\n",
            "Iteration 10, loss = 0.435, ber = 0.02387\n",
            "Iteration 20, loss = 0.411, ber = 0.02023\n",
            "Iteration 30, loss = 0.405, ber = 0.01934\n",
            "Iteration 40, loss = 0.417, ber = 0.02004\n",
            "Iteration 50, loss = 0.409, ber = 0.01960\n",
            "Iteration 60, loss = 0.405, ber = 0.01929\n",
            "Iteration 70, loss = 0.400, ber = 0.01902\n",
            "Iteration 80, loss = 0.409, ber = 0.01963\n",
            "Iteration 90, loss = 0.406, ber = 0.01949\n",
            "Iteration 100, loss = 0.408, ber = 0.01960\n",
            "Iteration 110, loss = 0.410, ber = 0.01943\n",
            "Iteration 120, loss = 0.410, ber = 0.01967\n"
          ]
        }
      ]
    }
  ]
}